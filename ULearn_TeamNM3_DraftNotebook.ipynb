{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e1ff526",
   "metadata": {
    "id": "3e1ff526"
   },
   "source": [
    "# Unsupervised Learning Predict - Movie Recommender System Challenge\n",
    "© Explore Data Science Academy\n",
    "\n",
    "---\n",
    "### Honour Code\n",
    "\n",
    "We, **XXX** {**#Team_NM3**}, confirm - by submitting this document - that the solutions in this notebook are a result of our own work and that we abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
    "\n",
    "Non-compliance with the honour code constitutes a material breach of contract."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db26ce12",
   "metadata": {
    "id": "db26ce12"
   },
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "#### Section 1: Data Pre-processing\n",
    "\n",
    "<a href=#one>1.1 Importing Packages</a>\n",
    "\n",
    "<a href=#two>1.2 Loading Data</a>\n",
    "\n",
    "<a href=#three>1.3 Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#four>1.4 Data Engineering</a>\n",
    "\n",
    "#### Section 2: Model Development and Analysis\n",
    "\n",
    "<a href=#five>2.1 Modeling</a>\n",
    "\n",
    "<a href=#six>2.2 Model Performance</a>\n",
    "\n",
    "#### Section 3: Model Explanation and Conclusions\n",
    "\n",
    "<a href=#seven>3.1 Model Explanations</a>\n",
    "\n",
    "<a href=#seven>3.2 Conclusions</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efcbcb3",
   "metadata": {
    "id": "7efcbcb3"
   },
   "source": [
    "# Introduction\n",
    "In today’s technology driven world, recommender systems are socially and economically critical for ensuring that individuals can make appropriate choices surrounding the content they engage with on a daily basis. One application where this is especially true surrounds movie content recommendations; where intelligent algorithms can help viewers find great titles from tens of thousands of options.\n",
    "\n",
    "Providing an accurate and robust solution to this challenge has immense economic potential, with users of the system being exposed to content they would like to view or purchase - generating revenue and platform affinity. \n",
    "\n",
    "This Notebook has been adapted and developed by **XXX** - a group of seven students from the July 2022 cohort of the Explore Ai Academy **Data Science** course. We are:\n",
    "\n",
    " > Josiah Aramide <br>\n",
    " > Bongani Mavuso <br>\n",
    " > Ndinannyi mukwevho <br>\n",
    " > Aniedi Oboho-Etuk <br>\n",
    " > Manoko Langa <br>\n",
    " > Tshepiso Padi <br>\n",
    " > Nsika Masondo <br>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c635e3",
   "metadata": {
    "id": "19c635e3"
   },
   "source": [
    "### Problem Statement\n",
    "\n",
    "The client is determined to improve its recommender system service offering to targeted consumer categories based on their movie content rating. \n",
    "\n",
    "Data from the historical viewing experiences, available to the company contains some preference and similarity characteristics that can ensure accurate prediction of consumer behaviour. \n",
    "\n",
    "By constructing a recommendation algorithm based on content or collaborative filtering, **XXX** team can develop a solution capable of accurately predicting how a user will rate a movie they have not yet viewed based on their historical preferences. This solution can give the company access to immense economic potential, with users of the system being exposed to content they would like to view or purchase - generating revenue and platform affinity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c657c7",
   "metadata": {
    "id": "15c657c7"
   },
   "source": [
    "### Objectives\n",
    "\n",
    "**XXX** seeks to achieve the following objectives for the project brief:\n",
    "\n",
    "- 1. analyse the supplied data;\n",
    "- 2. identify potential errors in the data and clean the existing data set;;\n",
    "- 3. determine if additional features can be added to enrich the data set;\n",
    "- 4. build a recommendation algorithm based on content or collaborative filtering that is capable of capable of accurately predicting how a user will rate a movie they have not yet viewed;\n",
    "- 5. evaluate the accuracy of the best machine learning model; and\n",
    "- 6. explain the inner working of the model to a non-technical audience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0e4c7a",
   "metadata": {
    "id": "eb0e4c7a"
   },
   "source": [
    "# Section 1: Data Pre-processing\n",
    "\n",
    "This section describes steps for installing dependencies and requirements, initializing the experiment on Comet, importing packages, loading the two datasets - train and test datasets, conducting the exploratory data analysis (EDA) and implementing data engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cac80eb",
   "metadata": {
    "id": "9cac80eb"
   },
   "source": [
    " <a id=\"one\"></a>\n",
    "## 1.1 Importing Packages\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Importing Packages ⚡ |\n",
    "| :--------------------------- |\n",
    "| Below are the libraries and tools imported for use in this project. The libraries include:\n",
    "   - **numpy**: for working with arrays,\n",
    "   - **pandas**: for tansforming and manipulating data in tables,\n",
    "   - **matplotlib**: for creating interactive visualisations,\n",
    "   - **seaborn**: for making statistical graphs and plots,\n",
    "   - **scikit-learn**: for machine learning and statistical modeling, and\n",
    "   - **math**: for algebraic notations and calculations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d540f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comet installation for Jupyter Notebook/Collab\n",
    "!pip install comet_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "ebba6e7e",
   "metadata": {
    "id": "ebba6e7e"
   },
   "outputs": [],
   "source": [
    "# Libraries for data loading, data manipulation and data visulisation \n",
    "import numpy as np   # for working with \n",
    "import pandas as pd  # for data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt  # for making visualisations and plots\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "\n",
    "# Libraries for collecting experiment parameters\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import comet_ml\n",
    "from comet_ml import Experiment\n",
    "\n",
    "# Libraries for data engineering and model building\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler # for standardization\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# Libraries for Building Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import mutual_info_regression #determine mutual info\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Libraries for model performance (metrics)\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "import math\n",
    "import time\n",
    "import datetime as dt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04a0116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an experiment with your api key\n",
    "experiment = Experiment(\n",
    "    api_key=\"xxxxxxxx\",\n",
    "    project_name=\"xxxxxxxxxxxx\",\n",
    "    workspace=\"teamnm3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16bd964",
   "metadata": {
    "id": "b16bd964"
   },
   "source": [
    "<a id=\"two\"></a>\n",
    "## 1.2 Loading the Data\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Loading the data ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, data is loaded from the **xxxxx** made available to **TeamNM3** by the client, **Explore-AI**. This involves reading the data from the `.csv` file format into a Pandas dataframe. The Pandas dataframe allows for easy views and manipulations of the data in the form of tables and can be combined with other python libraries like numpy for desirable results. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "yIS-Xiek0khY",
   "metadata": {
    "id": "yIS-Xiek0khY"
   },
   "outputs": [],
   "source": [
    "# Store datasets in a Pandas Dataframe\n",
    "df = pd.read_csv('xxx.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bdd2f3",
   "metadata": {
    "id": "54bdd2f3"
   },
   "source": [
    "<a id=\"three\"></a>\n",
    "## 1.3 Exploratory Data Analysis (EDA)\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Exploratory data analysis ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, an in-depth analysis (graphical and non-graphical) of the supplied data is conducted. This includes: \n",
    " - viewing the matrix to determine the dimensions of the data;\n",
    " - identifying the features and target;\n",
    " - investigating the formatting of the data (types, nulls etc.)\n",
    " - viewing the xxx;\n",
    " - identifying the xxx;\n",
    " - analysing the xxx;\n",
    " .|\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7a14a8",
   "metadata": {},
   "source": [
    "### 1.3.1 Viewing the matrix (dimensions) of the data\n",
    "First, it is necessary to view the matrix of the train and test datasets to see the total number of rows, cumner of columns, the content and the format (datatypes) of the features and labels that both datasets contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b2897e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1578b772",
    "outputId": "6c76578b-bdf1-45ed-f263-97001fb100db"
   },
   "outputs": [],
   "source": [
    "# Train dataset Matrix\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fcce02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset Matrix\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e603b2",
   "metadata": {},
   "source": [
    "### OBSERVATION\n",
    "- As the results show, the train dataset contains **15,819 rows of observations** in 3 columns of features and the target (or response) variable, \n",
    "- The test dataset contains a much lower number of observations (**10,546**) with only 2 columns i.e. not containing the target variable. \n",
    "\n",
    "Next, a peek view of some of the rows in the dataset should be of interest. This can be accomplished with the `pd.head()` command as seen in the code cell below. The command can take an argument specifying the number of rows to view (15 in this example), otherwise it returns the first 5 rows by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcf6f94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "H8-q8ZB8unwy",
    "outputId": "37e6ce69-1a94-4ca6-a2c1-f4a97c0a087f"
   },
   "outputs": [],
   "source": [
    "# View top of datasets, train set\n",
    "\n",
    "df_train.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90172aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the test set\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a292c",
   "metadata": {},
   "source": [
    "### OBSERVATION\n",
    "- The output indicates that the `xxx` column (features) contains xxx. These will need to be addressed during the feature engineering phase in order to derive any usefulness from them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c13a17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W8m9wlQ_3LHq",
    "outputId": "2db55cc2-fe39-435c-9a4b-d3d1a2b9dd6b"
   },
   "outputs": [],
   "source": [
    "# Data Types and Non-null count \n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcdaa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the Non-null count\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d676ae24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "QWa6XquV3K8r",
    "outputId": "10a111d5-e804-41cc-90c5-a42c49d702b2"
   },
   "outputs": [],
   "source": [
    "# Summary Statistics of our train dataset\n",
    "df_train.groupby(\" \").describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6XQcByLz3bDi",
   "metadata": {
    "id": "6XQcByLz3bDi"
   },
   "source": [
    "### OBSERVATION\n",
    "- From above, it can be observed that the dataset appears to have no missing values. That is, the count of non-null rows equals the expected count of entries in the columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68a19e1",
   "metadata": {},
   "source": [
    "### 1.3.2 Visualisation: Histogram of ... showing outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c159c1",
   "metadata": {},
   "source": [
    "### OBSERVATION\n",
    " - One immediately obvious fact from the unclean dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b86004",
   "metadata": {},
   "source": [
    "### 1.3.3 Visualisation: Distribution (density) plot of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e362ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the distribution of tweets in the data\n",
    "\n",
    "length_train = df_train[' '].str.len().plot.hist(color = 'green', figsize = (6, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d13a098",
   "metadata": {},
   "source": [
    "### OBSERVATION\n",
    "- The barplots above confirm the \n",
    "\n",
    "For now, this seems to be as much insight as can be displayed from the raw dataset. In the next stage of Data Engineering, the observations highlighted will be implemented particularly those that have to do with cleaning the message, removing outliers and noise, and manipulating the result into a format appropriate for use in machine learning models.\n",
    "\n",
    "The following section details how to achieve just that!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6098c567",
   "metadata": {
    "id": "6098c567"
   },
   "source": [
    "<a id=\"four\"></a>\n",
    "## 1.4 Data Engineering\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Data engineering ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section we conduct our feature engineering to: \n",
    "- clean identified errors from the dataset;\n",
    "- enrich the dataset by creating new features;\n",
    "- split the dataset into training and validation sets for use by selected models;\n",
    "- standardize the dataset;\n",
    "- \n",
    "- \n",
    "\n",
    "These steps follow the insights that were gathered earlier during the EDA phase.|\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7151f484",
   "metadata": {},
   "source": [
    "### 1.4.1 Preprocessing 1: Cleaning\n",
    "The first step is to begin organising the data cleaning exercise by building smart functions so that these can be recalled for cleaning both the training and testing datasets. Without this logical flow of cleaning the data, the exercise can quickly get very messy. However, with a couple of functions, it can be decided where the code lines will be inserted for repetitive tasks such as ... Then, the cleaning exercise can logically progress ... as shown below. \n",
    "\n",
    "Then, these functions are called to clean \n",
    "\n",
    "Later, in Step 2, ... ready for the modeling phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80f053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to do some preprocessing on the data\n",
    "def xxx(yyy):\n",
    "    '''\n",
    "    :parameter\n",
    "        :\n",
    "    :return\n",
    "        \n",
    "    '''\n",
    "    return xx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e4e26f",
   "metadata": {},
   "source": [
    "### 1.4.2 Preprocessing 2 - Split and Standardization\n",
    "In this step, the task is to complete preprocessing on the train and test datasets ahead of modeling. First, a function is created to split the datasets into train and validation sets to support the performance measurement during the modeling stage. Next, another function is created to standardize the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f17d45",
   "metadata": {},
   "source": [
    "#### 1.4.2.1 Splitting\n",
    "Create a `preprocess_train_split` function to complete the task of splitting features from the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47258e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to preprocess data for our models\n",
    "def preprocess_train_split(xxx):\n",
    "    '''\n",
    "    :parameter\n",
    "        :\n",
    "    :return\n",
    "        split of \n",
    "    '''\n",
    "    \n",
    "    # split train data into train and validation datasets\n",
    "        \n",
    "    return (X_train, y_train), (X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b635ea6c",
   "metadata": {},
   "source": [
    "#### 1.4.2.2 Extract the features\n",
    "Call the `preprocess_train_split` function to split/extract the features into actual variables for training, validation and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e12c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitted train dataset into training and validation sets\n",
    "(X_train, y_train), (X_valid, y_valid) = preprocess_train_split(xxx)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a60d95",
   "metadata": {},
   "source": [
    "#### 1.4.2.3 Standardizing the features\n",
    "Create a `stand` function to complete the task of standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "cea7d512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining global scalers\n",
    "rs = RobustScaler()\n",
    "mm = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "3c112e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stand(X_train, X_valid, X_test):\n",
    "    '''\n",
    "    :parameter\n",
    "        :\n",
    "    :return\n",
    "        \n",
    "    '''\n",
    "    # standardize the features to be in comparable scale\n",
    "    rs = RobustScaler()\n",
    "    mm = MinMaxScaler()\n",
    "    train_vect = rs.fit_transform(train_vect)\n",
    "    train_vect = mm.fit_transform(train_vect)\n",
    "    \n",
    "    return train_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac3801c",
   "metadata": {},
   "source": [
    "With this level of cleaning concluded, the model building and development stage follows next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e96f775",
   "metadata": {
    "id": "7e96f775"
   },
   "source": [
    "# Section 2: Model Development and Analysis\n",
    "\n",
    "This section describes the steps and processes involved in building models for the project as well as the analysis of the model performance in terms of their accuracy in accomplishing the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09d8e4b",
   "metadata": {
    "id": "f09d8e4b"
   },
   "source": [
    "# <a id=\"five\"></a>\n",
    "## 2.1 Modelling\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Modelling ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, the **TeamNM3** team explored the following models for their skill and strengths with regards processing  was considered in the model development. The models include:\n",
    "\n",
    "- 1. L\n",
    "- 2. S \n",
    "---\n",
    "The initial task is to build a base model using ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4b0d81",
   "metadata": {},
   "source": [
    "### 2.1.1 Overview of the Selected Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d9c203",
   "metadata": {},
   "source": [
    "### 2.1.2 Fit, Train and Predict with a base model\n",
    "The first step of modeling involved fitting, training and predicting a base model of ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4336e0",
   "metadata": {},
   "source": [
    "### DISCUSSION\n",
    "The two outputs above are "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961aad64",
   "metadata": {},
   "source": [
    "### 2.1.3 Building other models \n",
    "With the base model fully operational, it is now reasonable to develop other models that can strengthen the recommendation system task. As with all the earlier stages of the data science process, functions are built to help enhance the functionality of training and testing the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fa069a",
   "metadata": {},
   "source": [
    "#### 2.1.3.1 Create model objects for all models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48710566",
   "metadata": {},
   "source": [
    "#### 2.1.3.2 Create functions for training and testing all models\n",
    "Two functions `train_model` and `test_model` are created to optimize the process of training and testing all selected models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a447182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to train our models\n",
    "def train_model(model, X, y):\n",
    "\n",
    "    ''' returns a model trained on the training dataset\n",
    "        parameters:\n",
    "            model:   a machine learning model\n",
    "            X:\n",
    "    '''    \n",
    "    return model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda340f8",
   "metadata": {},
   "source": [
    "### 2.1.4 Model Fitting, Training and Predictions\n",
    "\n",
    "The models are fitted and trained on the balanced datasets and then used for predicting the tweet classification task on the unseen dataset. The process involves using the trained models by calling on built functions. \n",
    "\n",
    "First, the prediction is done with the validation dataset which has a label but has not been resampled. This prediction results are used in the next sub-section for evaluating the model performance. Another prediction set is conducted subsequently on the blind test dataset which has no labels. This prediction is used for the Kaggle submission to obtain external scores on the performance of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2cd20f",
   "metadata": {},
   "source": [
    "#### 2.1.4.1 Model 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "777cf048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf6be41",
   "metadata": {},
   "source": [
    "#### 2.1.4.2 Model 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5d4ec6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the support vector machine \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ddfd28",
   "metadata": {},
   "source": [
    "### 2.1.5 Extract Results for Submission\n",
    "With the model fitting, training and prediction tasks completed, it is now possible to extract results from some of the models for submission on Kaggle as well as for use in Streamlit web app development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee7fd70",
   "metadata": {},
   "source": [
    "#### Extracting Results for Submission - Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e62c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a Kaggle submission file for the model\n",
    "results_dict = pd.DataFrame({'tweetid':df_test['tweetid'],\n",
    "                'sentiment': nbc_org_pred_test})\n",
    "\n",
    "results_dict.to_csv('submission_nbc_org.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcbfef3",
   "metadata": {},
   "source": [
    "#### Extracting pkl file for web app development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510c38e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle/save base model for Streamlit web deployment\n",
    "model_save_path = \"lgr_base.pkl\"\n",
    "with open(model_save_path,'wb') as file:\n",
    "    pickle.dump(lgr_train,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db888d19",
   "metadata": {
    "id": "db888d19"
   },
   "source": [
    "<a id=\"six\"></a>\n",
    "## 2.2 Model Performance\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model performance ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, the relative performance of the selected classification models against some common metrics are compared and considered. The following metrics are deployed in checking the model performance using functions, as previously established:\n",
    "-  |\n",
    "\n",
    "---\n",
    "**xxx**\n",
    "\n",
    ".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fca059",
   "metadata": {},
   "source": [
    "### 2.2.1 Model Scores, Matrices and Heatmaps\n",
    "A function is built to take care of the `roc_auc_score` calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a927cd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for calculating roc scores\n",
    "def roc_score(model, X_valid, y_valid):    \n",
    "    # with the model previously instantiated, \n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facbe0c7",
   "metadata": {},
   "source": [
    "#### 2.2.1.1 Scores and Matrices of models trained on the balanced training dataset\n",
    "The scores of models trained on the resampled datasets are first verified and then tabulated and plotted for easy comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f80598",
   "metadata": {},
   "source": [
    "#### Model 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb926bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print roc_score for xxx model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac606e09",
   "metadata": {},
   "source": [
    "#### Model 2: Support Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c32b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot bar of roc\n",
    "roc_factsheet.plot(kind='bar', title='ROC scores across selected xxx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacec1df",
   "metadata": {},
   "source": [
    "### DISCUSSION\n",
    "In the simple barplot of the ROC scores above, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939cfe1c",
   "metadata": {},
   "source": [
    "### DISCUSSION\n",
    "In this instance,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ffef1b",
   "metadata": {},
   "source": [
    "### 2.2.2 Improving model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45c12f2",
   "metadata": {},
   "source": [
    "The results above ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a2abe2",
   "metadata": {},
   "source": [
    "#### 2.2.2.1 Implementing Hyperparameter tuning to improve model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dc0afd",
   "metadata": {
    "id": "12dc0afd"
   },
   "source": [
    "# Section 3: Model Explanations and Conclusions\n",
    "\n",
    "This section describes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dcea8f",
   "metadata": {
    "id": "63dcea8f"
   },
   "source": [
    "<a id=\"seven\"></a>\n",
    "## 3.1 Model Explanation\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model explanation ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, we discuss the inner workings of some of the selected models work in an attempt to understand how the models have performed the task. We discuss the following models:\n",
    "- \n",
    "- Support Vector Machines,\n",
    "- Random Forest.|\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfca17cb",
   "metadata": {},
   "source": [
    "### 3.1.1 Understanding the inner workings of select models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d388950",
   "metadata": {},
   "source": [
    "### 3.1.2 Characteristics and Advantages of the Best Performing Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995e6112",
   "metadata": {
    "id": "995e6112"
   },
   "source": [
    "<a id=\"seven\"></a>\n",
    "## 3.2 Conclusions\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model explanation ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, we draw conclusions and consider a few recommendations based on the discussions and investigations conducted for this Twitter classification project.|\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec42456",
   "metadata": {},
   "source": [
    "In conclusion, it can be said that:\n",
    "- the data available from the \n",
    "\n",
    "Finally, it is evident that deploying machine learning solutions that are well-tuned to \n",
    "\n",
    "Thus, thorough consideration of the strategic objectives and direction of the company with regards to interventions to be supported by insights from the ... can improve the choice of the machine learning model that best delivers on the recommendation system task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a1f9af",
   "metadata": {},
   "source": [
    "### 3.2.1 Logging and extracting parameters for Comet experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "56a21301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries for the data we want to log\n",
    "\n",
    "# metrics\n",
    "metrics_nbc_smt = {\"f1\": nbc_smt_f1, \"recall\": nbc_smt_r, \n",
    "                  \"precision\": nbc_smt_p, \"roc\": nbc_roc}\n",
    "\n",
    "# parameters\n",
    "params_nbc_smt = {\"vectorizer\": tf_vect, \"model_type\": \"naive bayes\", \n",
    "                 \"model\": nbc_smt, \"robust scaler\": rs, \"Min Max\": mm}\n",
    "\n",
    "#params_abc_sm = {\"random_state\": 42, \"vectorizer\": tf_vect, \n",
    " #                \"model_type\": \"ada boost\", \"model\": abc_sm, \n",
    " #                \"robust scaler\": rs, \"Min Max\": mm, \n",
    " #                 \"base_estimator\": rfc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "9d60af54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log our parameters and results\n",
    "experiment.log_parameters(params_nbc_smt)\n",
    "experiment.log_metrics(metrics_nbc_smt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc82ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# end the experiment on Comet\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e737c38",
   "metadata": {},
   "source": [
    "Running experiment.display() will show the experiments comet.ml page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "269d356b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800px\"\n",
       "            src=\"https://www.comet.com/teamnd2/classification-predict/9c254e53537b49c88b12ba1340defe63\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc8c82a63a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display the experiment parameters on Comet\n",
    "experiment.display()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[{"file_id":"https://github.com/joarams/UL_TeamNM3_VC/blob/main/ul-teamnm3-semi-final%20draftnotebook%2001.02.2022.ipynb","timestamp":1675251942047}]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Unsupervised Learning Predict - Movie Recommender System Challenge\n© Explore Data Science Academy\n\n---\n### Honour Code\n\nWe, **InfinityAI** {**#Team_NM3**}, confirm - by submitting this document - that the solutions in this notebook are a result of our own work and that we abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n\nNon-compliance with the honour code constitutes a material breach of contract.","metadata":{"id":"3e1ff526"}},{"cell_type":"markdown","source":"<a id=\"cont\"></a>\n\n## Table of Contents\n\n#### Section 1: Enviroment Setup\n\n<a href=#one>1.1 Python Package Setup</a>\n\n<a href=#two>1.2 Comet  Initialization</a>\n\n<a href=#three>1.3 Package Imports</a>\n\n#### Section 2: Data\n\n<a href=#five>2.1 Download of dataset</a>\n\n<a href=#six>2.2 Basic Data Analysis</a>\n\n#### Section 3: Exploratory Data Analysis\n\n<a href=#seven>3.1 Explore user data</a>\n\n<a href=#seven>3.2 Explore movie genre</a>\n\n<a href=#seven>3.3 Explore movie data</a>\n\n<a href=#seven>3.4 Explore imdb data</a>\n\n<a href=#seven>3.5 Explore tags data</a>\n\n<a href=#seven>3.6 Explore publishing years</a>\n\n\n###  Section 4: Base Model Testing\n\n<a href=#eight>4.1 Cross-Validation Testing</a>\n\n<a href=#nine>4.2  Train-Test-Split</a>\n\n<a href=#ten>4.3   Grid Search</a>\n\n###  Section 5: Model Building\n\n<a href=#twelve>5.1 Fit model to whole dataset</a>\n\n<a href=#thirteen>5.2 Download CSV for Kaggle Competition</a>\n\n<a href=#fourteen>5.3 Pickle model for use in Streamlit</a>\n\n### Section 6:  Collaborative & Content Based Model\n\n<a href=#fifteen>6.1 Collaborative Filtering - Approach I</a>\n\n<a href=#sixteen>6.2 Content Based - Approach II</a>\n\n### Section 7: Conclusion\n","metadata":{"id":"db26ce12"}},{"cell_type":"markdown","source":"# Introduction\nMovies have managed to enthrall audiences ever since one second clips of racing horses emerged in the 1890s to the introduction of sound in the 1920s to the birth of color in the 1930s to mainstream 3D Movies in the early 2010s. Around the world, movie industries have been blessed with creative geniuses in the form of directors, screenwriters, actors, sound designers and cinematographers. Together with the rise in popularity of portable devices, capable of hosting streaming services, movies have ensured that people can stay glued to their favourites whether in transit or in the corners of their homes. \n\nHowever, the spread into a plethora of genres ranging from romance to comedy to science fiction to horror has created a new problem of information overload, where choice and decision-making for individuals has become quite challenging. \n\nIn today’s technology driven world, there have been several attempts to solving this problem using recommender systems. These systems are basically a subclass of intelligent information filtering processes that provide suggestions for items that are most pertinent to a particular user.\n\nProviding an accurate and robust solution to this challenge has immense economic potential for industry clients, with users of the system being exposed to content they would like to view or purchase - generating revenue and platform affinity. \n\n![IMG_3532.png](attachment:1293c550-39df-42d0-bd59-6aba283d8dc9.png)\n\nIn this Notebook, the **Infinity AI** team identifies some insights into data that can be used for the development of a few recommender systems. The team explores eight datasets of more than 48000 movies and over 160000 users with up to 15 million of datapoints containing movie ratings, genres, keywords, and so on collected from Explore Ai Academy (EDSA) and the MovieLens datasets. Using these datasets, the team attempts to answer various questions about movies. We are:\n\n > Josiah Aramide <br>\n > Bongani Mavuso <br>\n > Ndinannyi mukwevho <br>\n > Aniedi Oboho-Etuk <br>\n > Manoko Langa <br>\n > Tshepiso Padi <br>\n > Nsika Masondo <br>\n ","metadata":{"id":"7efcbcb3"}},{"cell_type":"markdown","source":"### Problem Statement\n\nEXPLORE AI (the client) is determined to improve her recommender system service to targeted consumer categories based on their movie content ratings. \n\nData from the historical viewing experiences, available to the company contains some movie preference (ratings) and similarity characteristics (movie attributes) that can ensure accurate prediction of consumer behaviour. \n\nBy constructing a recommendation algorithm based on content or collaborative filtering, Infinity AI team can develop a solution capable of accurately predicting how a user will rate a movie they have not yet viewed based on their historical preferences. This solution will give the company access to immense economic opportunities and guarantee customer retention, with users of the system being exposed to content they would like to view or purchase. Additionally, this easy-to-deploy solution will deliver increase in customer click-through rate - generating revenue and platform affinity.\n\n![movie_recommender_system.jpg](attachment:65060f6b-9d0c-4d92-bcfa-a43baca1acd6.jpg)\n","metadata":{"id":"19c635e3"}},{"cell_type":"markdown","source":"### Objectives\n\n**InfinityAI** seeks to achieve the following objectives for the project brief:\n\n- 1. analyse the supplied data for interesting insights on movies;\n- 2. identify underlying patterns and standardize the data;\n- 3. determine if additional features can be added to enrich the data set;\n- 4. build a recommendation algorithm based on content or collaborative filtering that is capable of capable of accurately predicting how a user will rate a movie they have not yet viewed;\n- 5. evaluate the accuracy of the best machine learning model; and\n- 6. explain the inner working of the model to a non-technical audience.","metadata":{"id":"15c657c7"}},{"cell_type":"markdown","source":"# Section 1: Enviroment Setup\n\nLet's start by installing and importing required packages/libraries.","metadata":{"id":"eb0e4c7a"}},{"cell_type":"markdown","source":"<a id=\"one\"></a>\n## 1.1 Python Package Installation\n<a href=#cont>Back to Table of Contents</a>\n\nTo run this notebook, install the following root packages on your local machine:\n\n- matplotlib\n- nltk\n- numpy\n- pandas\n- plotly\n- scikit-learn\n- seaborn\n- surprise\n- Comet","metadata":{"id":"phBYklX1mmlA"}},{"cell_type":"code","source":"# --> uncomment these lines below if the dependent code cells do not run\n\n!pip install comet_ml # Comet installation for Jupyter Notebook/Collab\n!pip install git+https://github.com/microsoft/recommenders.git\n!pip install kneed # knee (/elbow) point detection for cluster optimization\n!pip install tf_slim\n!pip install downcast\n!pip install scikit-surprise\n!pip install ipython-autotime\n!pip install surprise","metadata":{"id":"YlCVYftymmlA","executionInfo":{"status":"ok","timestamp":1675251879042,"user_tz":-60,"elapsed":375,"user":{"displayName":"","userId":""}},"execution":{"iopub.status.busy":"2023-02-02T14:16:16.726272Z","iopub.execute_input":"2023-02-02T14:16:16.727453Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Collecting comet_ml\n  Downloading comet_ml-3.32.0-py3-none-any.whl (462 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.7/462.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: websocket-client<1.4.0,>=0.55.0 in /opt/conda/lib/python3.7/site-packages (from comet_ml) (1.3.3)\nRequirement already satisfied: sentry-sdk>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from comet_ml) (1.13.0)\nRequirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from comet_ml) (4.6.1)\nRequirement already satisfied: requests>=2.18.4 in /opt/conda/lib/python3.7/site-packages (from comet_ml) (2.28.1)\nCollecting everett[ini]>=1.0.1\n  Downloading everett-3.1.0-py2.py3-none-any.whl (35 kB)\nCollecting dulwich!=0.20.33,>=0.20.6\n  Downloading dulwich-0.21.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (504 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m504.4/504.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from comet_ml) (1.15.0)\nCollecting wurlitzer>=1.0.2\n  Downloading wurlitzer-3.0.3-py3-none-any.whl (7.3 kB)\nCollecting semantic-version>=2.8.0\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from comet_ml) (6.0.0)\nCollecting python-box<7.0.0\n  Downloading python_box-6.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting requests-toolbelt>=0.8.0\n  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: simplejson in /opt/conda/lib/python3.7/site-packages (from comet_ml) (3.18.1)\nRequirement already satisfied: wrapt>=1.11.2 in /opt/conda/lib/python3.7/site-packages (from comet_ml) (1.12.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from dulwich!=0.20.33,>=0.20.6->comet_ml) (4.1.1)\nRequirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.7/site-packages (from dulwich!=0.20.33,>=0.20.6->comet_ml) (1.26.14)\nCollecting configobj\n  Downloading configobj-5.0.8-py2.py3-none-any.whl (36 kB)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.18.1)\nRequirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (5.10.2)\nRequirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (21.4.0)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.18.4->comet_ml) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.18.4->comet_ml) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.18.4->comet_ml) (2022.12.7)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->comet_ml) (3.8.0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":" <a id=\"one\"></a>\n## 1.2 Comet Initialization\n<a class=\"anchor\" id=\"1.1\"></a>\n<a href=#cont>Back to Table of Contents</a>\n\nDue to the way Comet ties into other Maching Learning packages automatically to track certain features, it is required to be one of the first packages imported at the top of the notebook.","metadata":{"id":"v97zyHt0mmlC"}},{"cell_type":"code","source":"# Create an instance of Comet experiment with TeamNM3's API key\nfrom comet_ml import Experiment\nexperiment = Experiment(\n    api_key=\"RpnzF8DcMSor3mXqAfEQqsXjv\",\n    project_name=\"unsupervised-learning-predict\",\n    workspace=\"teamnm3\",\n)","metadata":{"id":"wjTmThpEmmlD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" <a id=\"one\"></a>\n## 1.3 Importing Packages\n<a class=\"anchor\" id=\"1.2\"></a>\n<a href=#cont>Back to Table of Contents</a>\n\n---\n    \n| ⚡ Description: Importing Packages ⚡ |\n| :--------------------------- |\n| Below are the libraries and tools imported for use in this project. The libraries include:\n   - **numpy**: for working with arrays,\n   - **pandas**: for tansforming and manipulating data in tables,\n   - **matplotlib**: for creating interactive visualisations,\n   - **seaborn**: for making statistical graphs and plots,\n   - **scikit-learn**: for machine learning and statistical modeling, and\n   - **math**: for algebraic notations and calculations.\n\n---","metadata":{"id":"9cac80eb"}},{"cell_type":"code","source":"# Libraries for data loading, data manipulation and data visulisation \n# Import our regular old heroes \nimport numpy as np\nimport pandas as pd\nimport datetime\nimport time\nimport re\nimport scipy as sp # <-- The sister of Numpy, used in our code for numerical efficientcy. \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.offline import init_notebook_mode, iplot, plot\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go\nimport plotly.graph_objs as go\nimport plotly.offline as pyo\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.preprocessing import StandardScaler # for standardization \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.decomposition import PCA\nfrom sklearn import (manifold, datasets, decomposition, ensemble,\n                     discriminant_analysis, random_projection, preprocessing)\n\nfrom sklearn import preprocessing as pp\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score, mean_squared_error\n\n# Additional packages\nimport warnings\nfrom collections import OrderedDict\nfrom datetime import date\n#from comet_ml import Experiment\n\n# Entity featurization and similarity computation\nfrom sklearn.metrics.pairwise import cosine_similarity \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom wordcloud import WordCloud, STOPWORDS\n\n# Libraries used during sorting procedures.\nimport operator # <-- Convienient item retrieval during iteration \nimport heapq # <-- Efficient sorting of large lists\n\nfrom surprise import (\n    NMF,\n    SVD,\n    BaselineOnly,\n    CoClustering,\n    Dataset,\n    KNNBasic,\n    NormalPredictor,\n    Reader,\n    SlopeOne,\n    SVDpp,\n)\nfrom surprise import accuracy\nfrom surprise.model_selection import GridSearchCV, cross_validate, train_test_split\n\n# Imported for our sanity\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"id":"ebba6e7e","executionInfo":{"status":"ok","timestamp":1675251806374,"user_tz":-60,"elapsed":740,"user":{"displayName":"","userId":""}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Section 2: Data","metadata":{"id":"1aOleH6cmmlF"}},{"cell_type":"markdown","source":"<a id=\"two\"></a>\n## 2.1 Download of dataset\n<a class=\"anchor\" id=\"1.2\"></a>\n<a href=#cont>Back to Table of Contents</a>\n\n---\n    \n| ⚡ Description: Loading the data ⚡ |\n| :--------------------------- |\n| In this section, data is loaded from **Kaggle** made available to **TeamNM3** by the client, **Explore-AI**. This involves reading the data from the `.csv` file format into a Pandas dataframe. The Pandas dataframe allows for easy views and manipulations of the data in the form of tables and can be combined with other python libraries like numpy for desirable results. |\n\n---","metadata":{"id":"b16bd964"}},{"cell_type":"code","source":"# Import Data\n\n# Kaggle base path\n#base_path = \"../input/edsa-recommender-system-predict/\"\nbase_path = \"/kaggle/input/edsa-movie-recommendation-predict/\"\n\n# # Local base path\n#base_path = \"../../edsa-recommender-predict/\"\n\ndf_ratings = pd.read_csv(base_path + \"train.csv\")\ndf_movies = pd.read_csv(base_path + \"movies.csv\")\ndf_imdb = pd.read_csv(base_path + \"imdb_data.csv\")\n\ndf_genome_scores = pd.read_csv(base_path + \"genome_scores.csv\")\ndf_genome_tags = pd.read_csv(base_path + \"genome_tags.csv\")\ndf_links = pd.read_csv(base_path + \"links.csv\")\ndf_tags = pd.read_csv(base_path + \"tags.csv\")\ndf_rating = pd.read_csv('/kaggle/input/movie-lens-small-latest-dataset/ratings.csv')\n\ndf_test = pd.read_csv(base_path + \"test.csv\")\nsample_submission = pd.read_csv(base_path + \"sample_submission.csv\")","metadata":{"id":"ziyI-VCImmlG","executionInfo":{"status":"error","timestamp":1675251818671,"user_tz":-60,"elapsed":498,"user":{"displayName":"","userId":""}},"outputId":"fa2d53b3-d0c2-4d7f-ecba-0543fcaefdec","colab":{"base_uri":"https://localhost:8080/","height":398},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"three\"></a>\n## 2.2 Basic Data Analysis \n<a class=\"anchor\" id=\"1.1\"></a>\n<a href=#cont>Back to Table of Contents</a>\n\nIn this section we perform a basic analysis of the data in the various CSVs to develop an understanding of the data we're able to work with. We conclude this basic data analysis by combining all the data into one dataframe and then continuing into a more in-depth analysis.","metadata":{"id":"OIfzLzT9mmlG"}},{"cell_type":"markdown","source":"### Ratings DataFrame\n\nWe begin this basic data analysis by examining the ratings dataframe.","metadata":{"id":"7DcdCE22mmlG"}},{"cell_type":"code","source":"# Display top 5 rows of dataframe\ndf_ratings.head()","metadata":{"id":"lGRFKqX6mmlG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gather information about the dataframe\ndf_ratings.info()","metadata":{"id":"lKl_dSxjmmlI","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if dataframe as any null values\ndf_ratings.isnull().sum()","metadata":{"id":"YKbQmoy8mmlI","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We then create a dataframe to display the count and percentage of each rating value in the dataset.","metadata":{"id":"q4RfBw6gmmlJ"}},{"cell_type":"code","source":"# Determining number of rows for each rating value\nrows_rating = df_ratings[\"rating\"].value_counts()\nrows_df_rating = pd.DataFrame({\"rating\": rows_rating.index, \"Rows\": rows_rating.values})\n\n# Determining percentage of rows for each rating value\npercentage_rating = round(df_ratings[\"rating\"].value_counts(normalize=True) * 100, 2)\npercentage_df_rating = pd.DataFrame(\n    {\"rating\": percentage_rating.index, \"Percentage\": percentage_rating.values}\n)\n\n# Joining row and percentage information\nratings_df_distribution = pd.merge(\n    rows_df_rating, percentage_df_rating, on=\"rating\", how=\"outer\"\n)\nratings_df_distribution.set_index(\"rating\", inplace=True)\nratings_df_distribution.sort_index(axis=0)","metadata":{"id":"yJWJj2lnmmlJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the dataframe above we can see that 4.0 is the most commonly score, with 26.53% of the movies in the dataframe assigned that score.\n\nWe visualize the data below:","metadata":{"id":"gLrxiL7kmmlJ"}},{"cell_type":"code","source":"pyo.init_notebook_mode()\ninit_notebook_mode(connected=True)\ndata = df_ratings[\"rating\"].value_counts().sort_index(ascending=False)\n\n# Plot data\ntrace = go.Bar(\n    x=data.index,\n    text=[\"{:.1f} %\".format(val) for val in (data.values / df_ratings.shape[0] * 100)],\n    textposition=\"auto\",\n    textfont=dict(color=\"#000000\"),\n    y=data.values,\n)\n\n# Create layout\nlayout = dict(\n    title=\"Distribution Of {} ratings\".format(df_ratings.shape[0]),\n    xaxis=dict(title=\"Rating\"),\n    yaxis=dict(title=\"Count\"),\n)\n\n# Create plot\nfig = go.Figure(data=[trace], layout=layout)\npyo.iplot(fig)","metadata":{"id":"bpJL9Bv-mmlJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we see half scores (0.5, 1.5, 2.5, 3.5 and 4.5) are less commonly used than integer score values. We don't know if this is because users prefer to rate movies with integer values or if it's because half scores were introduced after the original scoring system was already in use, leading to a decreased volume in a dataset with ratings from 1995. We quickly attempt to understand this further by investigating which years recorded half-score ratings:","metadata":{"id":"RTfCNbZWmmlK"}},{"cell_type":"code","source":"# Create list of date objects\nrating_date_list = [\n    date.fromtimestamp(timestamp) for timestamp in list(df_ratings[\"timestamp\"])\n]\n\n# Create year column\ndf_ratings[\"review_year\"] = [date_item.year for date_item in rating_date_list]\nyears_with_half_scores = df_ratings[\n    df_ratings[\"rating\"].isin([0.5, 1.5, 2.5, 3.5, 4.5])\n][\"review_year\"]\nunique_years_with_half_scores = set(years_with_half_scores)\nprint(\n    \"There are {} years with half scores. \\nThey are: {}.\".format(\n        len(unique_years_with_half_scores), sorted(list(unique_years_with_half_scores))\n    )\n)","metadata":{"id":"8cbwzcX_mmlK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that before 2003, movies were not rated with half scores.\n\nwe'll check the percentage of half scores of the total for the ratings from 2003 onwards:","metadata":{"id":"PP83crLommlK"}},{"cell_type":"code","source":"all_scores_after_2003 = len(df_ratings[\"rating\"])\nnumber_of_years_with_half_scores = len(years_with_half_scores)\nprint(\n    \"The percentage of reviews with half scores in the data from 2003 onwards is {:.2%}\".format(\n        number_of_years_with_half_scores / all_scores_after_2003\n    )\n)","metadata":{"id":"oDGK0n4MmmlL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that half scores are not as popular as integer scores.\n\n\nNow we examine the user data:","metadata":{"id":"qZbFi8twmmlL"}},{"cell_type":"code","source":"# Find the total number of users and movies and the number of unique users \n# and movies\nunique_users = df_ratings[\"userId\"].nunique()\ntotal_users = len(df_ratings[\"userId\"])\nunique_movies = df_ratings[\"movieId\"].nunique()\ntotal_movies = len(df_ratings[\"movieId\"])\n\n# Display these values\nprint(\n    \"Total number of unique users: \\t{} \\n\"\n    \"Total number of unique movies: \\t{}\\n\"\n    \"Percentage of unique users: \\t{:.2%}\\n\"\n    \"Percentage of unique movies: \\t{:.2%}\".format(\n        unique_users,\n        unique_movies,\n        unique_users / total_users,\n        unique_movies / total_movies,\n    )\n)","metadata":{"id":"gD12qFOXmmlL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The percentage of unique users and movies are both low.\n\n\nWe now move on to exploring the ratings dataframe.","metadata":{"id":"KASpBZe6mmlL"}},{"cell_type":"markdown","source":"**Summary of the basic analysis of the Ratings DataFrame**\n\nThe ratings dataframe consists of 10'000'038 rows and 4 columns (userId, movieID, rating and timestamp).\n\nRatings are from 0.5 to 5 in increments of 0.5. The majority of ratings were a 4, comprising of close to 27% of all the given data.\n\nThe data contains 162'541 unique users and 48'213 movies were rated.\n\nInteger scores appear favoured over half scores, which were only introduced in 2003","metadata":{"id":"UWGZEFVimmlL"}},{"cell_type":"markdown","source":"**Movies DataFrame**\n\nHere we perform a basic analysis of the movies dataframe. We begin this analysis by generating the head of the dataframe below:","metadata":{"id":"oaNgN0OhmmlM"}},{"cell_type":"code","source":"# Display top 5 rows of dataframe\ndf_movies.head()","metadata":{"id":"kyaVkEhcmmlM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gather information about the dataframe\ndf_movies.info()","metadata":{"id":"jIFLQN4NmmlM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's examine the number of unique values in the dataset:","metadata":{"id":"p0ITk_COmmlM"}},{"cell_type":"code","source":"# Information regarding number of unique values in each column:\nprint(\n    \"Total number of unique movie IDs: \\t{}\\n\"\n    \"Total number of unique movie titles: \\t{}\\n\"\n    \"Total number of unique movie genres: \\t{}\\n\".format(\n        df_movies[\"movieId\"].nunique(),\n        df_movies[\"title\"].nunique(),\n        df_movies[\"genres\"].nunique(),\n    )\n)\nprint(\n    \"There are {} movies with the same name.\".format(\n        df_movies[\"movieId\"].nunique() - df_movies[\"title\"].nunique()\n    )\n)","metadata":{"id":"4uNNt7uWmmlM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are fewer unique movie titles than unique IDs. We know that the difference between these two numbers is equal to the number of movies with the exact same name, which we see is 98. We will keep this in mind while building the recommender system.\n\nWe explore the genres column, first by finding the top 20 and lowest 20 genres by volume:","metadata":{"id":"cqm7iD1SmmlN"}},{"cell_type":"code","source":"# Top 20 genres by volume:\ndf_genres = df_movies[\"genres\"].value_counts()\ndf_genres.head(10)","metadata":{"id":"l0DluKjfmmlN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bottom 20 genres by volume:\ndf_genres = df_movies[\"genres\"].value_counts()\ndf_genres.tail(20)","metadata":{"id":"brn-tHUImmlN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the top genres by volume only have one or two genre types, whereas the bottom genres consist of multiple genres\n\n**Summary of the basic analysis of the Movies DataFrame**\n\nThe movies dataframe contains 62'423 rows and 3 columns (movieId, title and genres). 98 of the rows have duplicate titles. 1'639 unique genres are listed, which includes combination genres. 5062 movies do not have a genre listed and 3 most popular genres are: Drama, Comedy and Documentary.\n\nNext we move to the IMBD dataset.\n\n**IMDB Data**\n\nHere we explore the IMBD data to learn more about the content of the movies and the people who worked on them.\n\nWe begin by examining the dataframe.","metadata":{"id":"O5MlAtnammlO"}},{"cell_type":"code","source":"# Display top 5 rows of dataframe\ndf_imdb.head()","metadata":{"id":"0bFulMpsmmlO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gather information about the dataframe\ndf_imdb.info()","metadata":{"id":"9knD_y4cmmlO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we see that there are only 27277 movies in this dataframe, which is less than the 48213 movies in the ratings dataframe.\n\nNext we check for missing data:","metadata":{"id":"GVKbiLhGmmlO"}},{"cell_type":"code","source":"# Find percentage of missing values in each column\ncolumns = df_imdb.columns\npercent_missing_values = df_imdb.isnull().sum() / len(df_imdb.index) * 100\ndf_missing_values = pd.DataFrame(\n    {\"column_name\": columns, \"percent_missing\": percent_missing_values}\n)\ndf_missing_values","metadata":{"id":"aoB0xBKcmmlP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we see many columns are missing data.\n\nNext we examine the number of unique items in each column, keeping in mind that all columns except for the movie ID column is missing data:","metadata":{"id":"sZt0GFL4mmlP"}},{"cell_type":"code","source":"# Information regarding number of unique values in certain column:\nprint(\n    \"Total number of unique movie IDs: \\t{}\\n\"\n    \"Total number of unique title casts: \\t{}\\n\"\n    \"Total number of unique directors: \\t{}\\n\"\n    \"Total number of unique plot keywords: \\t{}\".format(\n        df_imdb[\"movieId\"].nunique(),\n        df_imdb[\"title_cast\"].nunique(),\n        df_imdb[\"director\"].nunique(),\n        df_imdb[\"plot_keywords\"].nunique(),\n    )\n)","metadata":{"id":"Becjbe7qmmlP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we check the dataframe for duplicated movies:","metadata":{"id":"A21HaUbFmmlP"}},{"cell_type":"code","source":"df_imdb[df_imdb[\"movieId\"].duplicated()]","metadata":{"id":"ki-nRKnommlP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"let's examine the most common cast members, directors, and plot keywords by volume in the dataset.","metadata":{"id":"3Un1Y1d1mmlQ"}},{"cell_type":"code","source":"# Top 5 title cast members by volume:\ndf_cast = df_imdb[\"title_cast\"].value_counts()\ndf_cast.head()","metadata":{"id":"UGVcIZ01mmlQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Top 5 directors by volume:\ndf_directors = df_imdb[\"director\"].value_counts()\ndf_directors.head()","metadata":{"id":"0n626juzmmlQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Top 5 plot keywords by volume:\ndf_keywords = df_imdb[\"plot_keywords\"].value_counts()\ndf_keywords.head()","metadata":{"id":"bZJoV2lDmmlQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Summary of the basic analysis of the IMDB DataFrame**\n\nThe IMDB Dataframe has 27'278 rows and 6 columns (movieId, title_cast, director, runtime, budget and plot_keywords). MovieId is the only column that doesn't have any null values. All the other columns have at least 36% of missing values, with the budget column having the hightest percentage of null values at 71%. No movieId's are duplicated.\n\nLuc Besson, Woody Allen and Stephen King are the 3 directors that appear most often in this dataset.\n\nThe most popular plot keywords are \"documentary\", \"action\", and \"f rated\".","metadata":{"id":"XxCaOWO5mmlR"}},{"cell_type":"markdown","source":"**Genome Scores DataFrame**\n\nHere we explore the genome scores data. This dataset contains scores that measure the relevance of a tag to a movie.","metadata":{"id":"03_4NpIHmmlR"}},{"cell_type":"code","source":"# Display top 5 rows of dataframe\ndf_genome_scores.head()","metadata":{"id":"ugmL3mTummlR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gather information about the dataframe\ndf_genome_scores.info()","metadata":{"id":"aaxwC5g4mmlR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Information regarding number of unique values in each column:\nprint(\n    \"Total number of unique movie IDs: \\t\" + str(df_genome_scores[\"movieId\"].nunique())\n)\nprint(\"Total number of unique tag IDs: \\t\" + str(df_genome_scores[\"tagId\"].nunique()))","metadata":{"id":"8LE5p12zmmlR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Summary of the basic analysis of the Genome Scores DataFrame**\n\nThe Genome Scores Dataframe has 15'584'448 rows and 3 columns (movieId, tagId and relevance). There are 13'816 unique movie ids and 1'128 unique tag ids.","metadata":{"id":"XdjRFrtKmmlS"}},{"cell_type":"markdown","source":"**Genome Tags DataFrame**\n\nHere we explore the tag data. These tags are assigned by a user.","metadata":{"id":"2m2wLwyTmmlS"}},{"cell_type":"code","source":"# Display top 5 rows of dataframe\ndf_genome_tags.head()","metadata":{"id":"bsHFvTNOmmlS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gather information about the dataframe\ndf_genome_tags.info()","metadata":{"id":"SymuMcQfmmlS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Information regarding number of unique values in each column:\nprint(\"Total number of unique Tags: \\t\\t\" + str(df_genome_tags[\"tag\"].nunique()))\nprint(\"Total number of unique Tag IDs: \\t\" + str(df_genome_tags[\"tagId\"].nunique()))","metadata":{"id":"OlkSBTurmmlS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Summary of the basic analysis of the Genome Tags DataFrame**\n\nThe Genome Tags Dataframe has 1'128 rows and 2 columns (tagId and tag). All values are unique.","metadata":{"id":"ykfmSwWcmmlS"}},{"cell_type":"markdown","source":"**Links DataFrame**\n\nHere we explore the links dataframe. From Kaggle, this data serves as a link between a MovieLens ID and the IMDB and TMDB IDs associated with it","metadata":{"id":"vyn01jAcmmlT"}},{"cell_type":"code","source":"# Display top 5 rows of dataframe\ndf_links.head()","metadata":{"id":"igmOm9apmmlT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gather information about the dataframe\ndf_links.info()","metadata":{"id":"jGwxFnGdmmlT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Total number of unique Movie Ids: \\t\" + str(df_links[\"movieId\"].nunique()))\nprint(\"Total number of unique IMDB Ids: \\t\" + str(df_links[\"imdbId\"].nunique()))\nprint(\"Total number of unique TMDB Ids: \\t\" + str(df_links[\"tmdbId\"].nunique()))","metadata":{"id":"S11bqE1QmmlT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_links.isnull().sum()","metadata":{"id":"J2ePOrdmmmlT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check how many tmdb IDs are duplicated:\ndf_tmdb = df_links[df_links[\"tmdbId\"].duplicated()]\ntmdb_total = df_tmdb[\"tmdbId\"].value_counts().sum()\nprint(\"Total number of tmdb Ids duplicated: \\t\" + str(tmdb_total))","metadata":{"id":"JMZykYQVmmlT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Summary of the Links DataFrame**\n\nThe links dataframe has 62'423 rows and 3 columns (movieId, imdbId and tmdbId). There are 62'423 unique movie and imdb IDs, which correspond with the number of rows in the dataframe. 107 of the tmdbId's are null values and 35 values are also dupclicates.","metadata":{"id":"G7lFjTXDmmlU"}},{"cell_type":"markdown","source":"**Test and sample submission Data**\n\nHere we briefly explore the data provided by Kaggle that users use for submissions.","metadata":{"id":"T-jCh6a2mmlU"}},{"cell_type":"code","source":"# Display top 5 rows of dataframe\ndf_test.head()","metadata":{"id":"mA4Ry_77mmlU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gather information about the dataframe\ndf_test.info()","metadata":{"id":"tE2K5jlQmmlU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display top 5 rows of dataframe\nsample_submission.head()","metadata":{"id":"c-yawxwgmmlU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gather information about the dataframe\nsample_submission.info()","metadata":{"id":"Ig0alBLjmmlV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission[sample_submission[\"Id\"].duplicated()]","metadata":{"id":"eCqBVT34mmlV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Summary of the test and sample submission Data**\n\nBoth dataframes have 5'000'019 and no IDs are ducplicated.","metadata":{"id":"DA9nyNmMmmlV"}},{"cell_type":"markdown","source":"# Section 3: Exploratory Data Analysis","metadata":{"id":"UWYa7uiVmmlV"}},{"cell_type":"markdown","source":"<a id=\"three\"></a>\n## 3.1 Exploring user data  \n<a class=\"anchor\" id=\"1.1\"></a>\n<a href=#cont>Back to Table of Contents</a>","metadata":{"id":"-V8k2yYpmmlV"}},{"cell_type":"markdown","source":"In this section we aim to explore the data specific to the users who contributed ratings.\n\nWe start this EDA by generating summary statistics for the rating values:","metadata":{"id":"9R0PoMhBmmlV"}},{"cell_type":"code","source":"# Generate summary statistics\nsummary_statistics = df_ratings[[\"rating\"]].describe().round(2)\nsummary_statistics","metadata":{"id":"ET_3E8CrmmlW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that the average is 3.53, which seems sensible for movie reviews with a maximum score of 5 and a minimum score of 0.5.","metadata":{"id":"YR9Q-YHjmmlW"}},{"cell_type":"markdown","source":"We find the number of times a user rated a movie","metadata":{"id":"9mg59TR8mmlW"}},{"cell_type":"code","source":"# To find the number of times a user rated a movie, we create a data frame with the count by userId\ndf_user = pd.DataFrame(\n    df_ratings['userId'].value_counts()).reset_index()\ndf_user.rename(columns={'index':'userId','userId':'count'},\n                  inplace=True)\ndf_user.head()","metadata":{"id":"wEHWMOsVmmlW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now aggregate user data by user IDs to get the average ratings.","metadata":{"id":"ifLnsoStmmlW"}},{"cell_type":"code","source":"df_aggregated = (\n    df_ratings[[\"userId\", \"rating\"]].groupby(\"userId\").agg([\"count\", \"mean\"])\n)\ndf_aggregated.head(5)","metadata":{"id":"3Z75pzs3mmlX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now group users in ranges and visualize the results","metadata":{"id":"B5WVJIbMmmlX"}},{"cell_type":"code","source":"# Grouping the users within a certain range aided us in determining the common userId's and the new ones.\ngroup_one = df_user.loc[(df_user['count'] > 0) & \n            (df_user['count'] < 50),\n            'userId'].value_counts().sum()\ngroup_two = df_user.loc[(df_user['count'] >= 50) & \n            (df_user['count'] < 500),\n            'userId'].value_counts().sum()\ngroup_three = df_user.loc[(df_user['count'] >= 500) & \n            (df_user['count'] < 1000),\n            'userId'].value_counts().sum()\ngroup_four = df_user.loc[(df_user['count'] >= 1000) & \n            (df_user['count'] < 1500),\n            'userId'].value_counts().sum()\ngroup_five = df_user.loc[(df_user['count'] >= 1500),\n            'userId'].value_counts().sum()","metadata":{"id":"UjP8ehiymmlX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualize the results","metadata":{"id":"1Z9DtkDBmmlX"}},{"cell_type":"code","source":"# To give us insight in the spread, we used figures to determine the spread.\ntrial_error = np.array([['group_one', group_one,\n                         'between 1 and 50'],\n                        ['group_two', group_two,\n                         'between 50 and 500'],\n                        ['group_three', group_three,\n                         'between 500 and 1000'],\n                        ['group_four', group_four,\n                         'between 1000 and 1500'],\n                        ['group_five', group_five,\n                         'greater than 1500']])\ndf_trial_error = pd.DataFrame({'group': trial_error[:, 0],\n                               'userId_grouping': trial_error[:, 1],\n                               'explanation': trial_error[:, 2]})\nfig = px.bar(df_trial_error,\n             x=df_trial_error[\"group\"],\n             y=df_trial_error[\"userId_grouping\"],\n             color=df_trial_error[\"group\"],\n             title='Grouped Rating Distribustion')\nfig.update_layout(legend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n    y=1.02,\n    xanchor=\"right\",\n    x=1\n))\nfig.show()\ndf_trial_error","metadata":{"id":"Hbkpe2s9mmlX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The user Id's are grouped by the rating counts in a grouping range illustrated in the DataFrame above. In the Grouped Rating Distribution bar graph, it is visually displayed that there is unequal distribution. The distribution is skewed to the left, with the majority of the user ids in the rating count range between 1 and 50. At the same time, the last group has only a value count of 61, which is a significant difference from group one with a value count of 110 010.","metadata":{"id":"FGR11ztQmmlX"}},{"cell_type":"code","source":"def user_ratings_count(df, n):\n    plt.figure(figsize=(14,7))\n    data = df['userId'].value_counts().head(n)\n    ax = sns.barplot(x = data.index, y = data, order= data.index, palette='CMRmap', edgecolor=\"black\")\n    for p in ax.patches:\n        ax.text(p.get_x() + p.get_width()/2., p.get_height(), '%d' % int(p.get_height()), fontsize=11, ha='center', va='bottom')\n    plt.title(f'Top {n} Users by Number of Ratings', fontsize=14)\n    plt.xlabel('User ID')\n    plt.ylabel('Number of Ratings')\n    plt.show()","metadata":{"id":"sotR-psHmmlY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_ratings_count(df_ratings,10)","metadata":{"id":"qssO7wXjmmlY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the graph above we can see there is one outlier with close to thirteen thousand reviews.","metadata":{"id":"68n3m7SVmmlY"}},{"cell_type":"markdown","source":"Filtering out user 72315 because his/her number of raings is too extreme and he/she is an oulier","metadata":{"id":"GcCQ49tlmmlY"}},{"cell_type":"code","source":"user_ratings_count(df_ratings[df_ratings['userId'] !=72315],10)","metadata":{"id":"qwRj0E1tmmlY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"three\"></a>\n## 3.2 Exploring movie genre  \n<a class=\"anchor\" id=\"1.1\"></a>\n<a href=#cont>Back to Table of Contents</a>","metadata":{"id":"NmVH5KrsmmlZ"}},{"cell_type":"code","source":"genres = pd.DataFrame(df_movies['genres'].\n                      str.split(\"|\").\n                      tolist(),\n                      index=df_movies['movieId']).stack()\ngenres = genres.reset_index([0, 'movieId'])\ngenres.columns = ['movieId', 'Genre']\ngenres.head()","metadata":{"id":"qhYrom05mmlZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(14, 7))\n\nsns.countplot(x='Genre',\n              data=genres,\n              palette='CMRmap',\n              order=genres['Genre'].\n              value_counts()[:10].index[::-1], #sliced to 10 for readability,\n             )\nplt.xticks(rotation=90)\nplt.xlabel('Genre', size=20)\nplt.ylabel('Count', size=20)\nplt.title('Distribution of Movie Genres', size=25)\nplt.show()","metadata":{"id":"pqPOF5yfmmlZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the top 3 most popular movie genres include drama, comedy and thriller.","metadata":{"id":"80s0ubwtmmlZ"}},{"cell_type":"markdown","source":"<a id=\"three\"></a>\n## 3.3 Exploring movie data  \n<a class=\"anchor\" id=\"1.1\"></a>\n<a href=#cont>Back to Table of Contents</a>","metadata":{"id":"jUhxrULUmmlZ"}},{"cell_type":"code","source":"# Merging ratings with movies data\n\nmovies=pd.merge(df_ratings, df_movies,on='movieId',how='inner')\nmovies.head()","metadata":{"id":"0uL4icKzmmlZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merging movies and imdb data\n\nfull_movies = pd.merge(movies,df_imdb,on='movieId',how='inner')\nfull_movies.head()","metadata":{"id":"Rn6TOoKGmmla","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def top_n_plot_by_ratings(df,column, n):\n    plt.figure(figsize=(14,7))\n    data = df[str(column)].value_counts().head(n)\n    ax = sns.barplot(x = data.index, y = data, order= data.index,\n                     palette='CMRmap', edgecolor=\"black\")\n    for p in ax.patches:\n        ax.text(p.get_x() + p.get_width()/2., p.get_height(), '%d' % int(p.get_height()), fontsize=11, ha='center', va='bottom')\n    plt.title(f'Top {n} {column.title()} by Number of Ratings', fontsize=14)\n    plt.xlabel(column.title())\n    plt.ylabel('Number of Ratings')\n    plt.xticks(rotation=90)\n    plt.show()","metadata":{"id":"X-Frc-Rgmmla","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# top 10 most rated movies\n\ntop_n_plot_by_ratings(movies,'title',10)","metadata":{"id":"6X-Fxrz5mmla","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the Top 15 Title by Number of Ratings bar graph, all the movies are prior the year 2001, with 14 of them released in the 19th century.\n\nThe top three are Shawshank Redemption 1994, Forest Grump 1994 and Pulp Fiction 1994. All three movies fall under the popular drama genre and are American.","metadata":{"id":"xRMoqCJSmmla"}},{"cell_type":"code","source":"# Wordcloud of movie titles\nmovies_word = df_movies['title'] = df_movies['title'].astype('str')\nmovies_wordcloud = ' '.join(movies_word)\ntitle_wordcloud = WordCloud(stopwords = STOPWORDS,\n                            background_color = 'White',\n                            height = 1200,\n                            width = 900).generate(movies_wordcloud)\nplt.figure(figsize = (14,7), facecolor=None)\nplt.imshow(title_wordcloud)\nplt.axis('off')\nplt.title('Distribution of words from movie titles')\nplt.tight_layout(pad=0)\nplt.show()","metadata":{"id":"zets6X_tmmla","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show the most rated rating\n\ntop_n_plot_by_ratings(movies,'rating',10)","metadata":{"id":"unpuOW1qmmla","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The most common rating score that is given is 4.0, followed by 3.0. The least common score that is given by usrs is 0.5.","metadata":{"id":"c5JuVPh-mmlb"}},{"cell_type":"code","source":"# show director's ratings\n\ntop_n_plot_by_ratings(full_movies,'director',10)","metadata":{"id":"eH3LW6aLmmlb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Quentin Tarantino is the top director of the number of ratings.","metadata":{"id":"lZmNnkGOmmlb"}},{"cell_type":"code","source":"movieRatingDistGroup = df_ratings['rating'].value_counts().sort_index().reset_index()\nfig, ax = plt.subplots(figsize=(14,7))\nsns.barplot(data=movieRatingDistGroup, x='index', y='rating', palette=\"CMRmap\", edgecolor=\"black\", ax=ax)\nax.set_xlabel(\"Rating\")\nax.set_ylabel('Number of Users')\nax.set_yticklabels(['{:,}'.format(int(x)) for x in ax.get_yticks().tolist()])\ntotal = float(movieRatingDistGroup['rating'].sum())\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2., height+350, '{0:.2%}'.format(height/total), fontsize=11, ha=\"center\", va='bottom')\nplt.title('Number of Users Per Rating', fontsize=14)\nplt.show()","metadata":{"id":"irDT50SImmlb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the users are weighted within the score range of 3.0 - 5.0, with the most users being weighted in the 4.0 score, accounting for 26.53% of the users.","metadata":{"id":"S2l1Q3PWmmlb"}},{"cell_type":"markdown","source":"<a id=\"three\"></a>\n## 3.4 Exploring imdb data  \n<a class=\"anchor\" id=\"1.1\"></a>\n<a href=#cont>Back to Table of Contents</a>","metadata":{"id":"RrNv2EfNmmlb"}},{"cell_type":"code","source":"def count_directors(df, count = 10):\n    \"\"\"\n    Function to count the most common dircetors in a DataFrame:\n    Parameters\n    ----------\n        df (DataFrame): input dataframe containing imdb metadata\n        count (int): filter directors with fewer than count films\n        \n    Returns\n    -------\n        directors (DataFrame): output DataFrame\n    Examples\n    --------\n        >>> df = pd.DataFrame({'imdbid':[0,1,2,3,4,5], 'director': [A,B,A,C,B]})\n        >>> count_directors(df, count = 1)\n            |index|director|count|\n            |0|A|2|\n            |1|B|2|\n            |2|C|1|\n    \"\"\"\n    directors = pd.DataFrame(df['director'].value_counts()).reset_index()\n    directors.columns = ['director', 'count']\n    # Lets only take directors who have made 10 or more movies otherwise we will have to analyze 11000 directors\n    directors = directors[directors['count']>=count]\n    return directors.sort_values('count', ascending = False)","metadata":{"id":"-Uv7adEymmlc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_count(df, column):\n    plt.figure(figsize=(14,7))\n    ax = sns.barplot(x = df[f'{column}'], y= df['count'], palette='brg')\n    for p in ax.patches:\n        ax.text(p.get_x() + p.get_width()/2., p.get_height(), '%d' % int(p.get_height()), fontsize=11, ha='center', va='bottom')\n    plt.title(f'Number of Movies Per {column}', fontsize=14)\n    plt.xlabel(f'{column}')\n    plt.ylabel('Count')\n    plt.xticks(rotation=90)\n    plt.show()","metadata":{"id":"TnSAIrcUmmlc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shows number of movies per director\n\ndirectors = count_directors(df_imdb)\nfeature_count(directors.head(11)[1:], 'director')","metadata":{"id":"UdDb0SFhmmlc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the Number of Movies Per Director bar graph, the leading director has produced the most movies with a count of 28. Luc Besson and Woody Allen are tied with a value count of producing 26 movies and followed by Stephan King with 24. They are the only producers in the dataset with over 20 movie productions.","metadata":{"id":"ASQ1gOEimmlc"}},{"cell_type":"markdown","source":"<a id=\"three\"></a>\n## 3.5 Exploring tags  \n<a class=\"anchor\" id=\"1.1\"></a>\n<a href=#cont>Back to Table of Contents</a>","metadata":{"id":"rOa_DI1Nmmlc"}},{"cell_type":"markdown","source":"First we create a word cloud of tags:","metadata":{"id":"I_CSaNf0mmlc"}},{"cell_type":"code","source":"comment_words = ''\nstopwords = set(STOPWORDS)\n\n# iterate through the csv file\nfor val in df_tags['tag']:\n\n    # typecaste each val to string\n    val = str(val)\n\n    # split the value\n    tokens = val.split()\n\n    # Converts each token into lowercase\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n\n    comment_words += \" \".join(tokens)+\" \"\n  \nwordcloud = WordCloud(width=1200, height=900,\n                      colormap='winter',\n                      background_color='white',\n                      stopwords=stopwords,collocations=False,\n                      min_font_size=10).generate(comment_words)\n\n# plot the WordCloud image\nplt.figure(figsize=(14, 7), facecolor=None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.title('Distribution of words in the tags data frame by Tags')\nplt.tight_layout(pad=0)\n\nplt.show()","metadata":{"id":"K2-POMJ2mmld","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see from the word cloud that the most common words in tags were 'Comedy','book','War' and 'Dark'.","metadata":{"id":"ChMfg9azmmld"}},{"cell_type":"code","source":"# creating a dataframe of genre and count of t\n\nvalue_count = pd.DataFrame(df_tags['tag'].\n                           value_counts()).reset_index()\nvalue_count.rename(columns = {'index': 'genre', 'tag': 'count'},\n                   inplace = True)","metadata":{"id":"H2soynMhmmld","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"value_count.head()","metadata":{"id":"StmWgqdemmle","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"genre_count = value_count.head(20)\nplt.figure(figsize=(14,7))\nax = sns.barplot(x = genre_count['genre'], y= genre_count['count'], palette='CMRmap')\nfor p in ax.patches:\n        ax.text(p.get_x() + p.get_width()/2., p.get_height(), '%d' % int(p.get_height()), fontsize=11, ha='center', va='bottom')\nplt.title('Number of times a genre tag appears', fontsize=14)\nplt.xlabel('Genre tag')\nplt.ylabel('Genre tag Count')\nplt.xticks(rotation=90)\nplt.show()","metadata":{"id":"xzHTIEXSmmle","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The most popular words in the world cloud include book, comedy, ending, based, dark and sci-fi.\nThe three most popular genres that appear in df_tags('tags') are sci-fi, atmospheric, and action.","metadata":{"id":"tifT0N-Ummle"}},{"cell_type":"markdown","source":"<a id=\"three\"></a>\n## 3.6 Publishing years \n<a class=\"anchor\" id=\"1.1\"></a>\n<a href=#cont>Back to Table of Contents</a>","metadata":{"id":"Rj_R1Qlnmmle"}},{"cell_type":"code","source":"dates = []\nfor title in df_movies['title']:\n    if title[-1] == \" \":\n        year = title[-6: -2]\n        try:\n            dates.append(int(year))\n        except:\n            dates.append(9999)\n    else:\n        year = title[-5: -1]\n        try:\n            dates.append(int(year))\n        except:\n            dates.append(9999)\n\ndf_movies['Publish Year'] = dates","metadata":{"id":"-S9RR8d5mmlf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nlen(df_movies)","metadata":{"id":"wi6k-V2Wmmlf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_movies[df_movies['Publish Year'] == 9999])","metadata":{"id":"WOvaic7Bmmlg","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_movies[(df_movies['Publish Year'] > 1888) &\n          (df_movies['Publish Year'] < 2021)]","metadata":{"id":"lAz450Kummlg","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = pd.DataFrame(df_movies['Publish Year'].\n                       value_counts()).reset_index()\ndataset.rename(columns={'index': 'year', 'Publish Year': 'count'},\n               inplace=True)\ndataset.head()","metadata":{"id":"LmWj2-Ctmmlg","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"year_dataset = dataset[(dataset['year']>1888) & (dataset['year']<2021)].sort_values(by='count',ascending=False).head(50)\nplt.figure(figsize=(14,7))\nax = sns.barplot(x = year_dataset['year'], y= year_dataset['count'], order=year_dataset['year'], palette='CMRmap')\n#for p in ax.patches:\n#       ax.text(p.get_x() + p.get_width()/2., p.get_height(), '%d' % int(p.get_height()), fontsize=11, ha='center', va='bottom')\nplt.title('Number of Movies Released Per year', fontsize=14)\nplt.xlabel('year')\nplt.ylabel('Released Movie Count')\nplt.xticks(rotation=90)\nplt.show()","metadata":{"id":"irJ3zMmammlg","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the Number of Movies Released Per year graph, we are able to visually see an major increase in movie releases in the 21st century.","metadata":{"id":"LmXhKVi7mmlh"}},{"cell_type":"markdown","source":"# Section 4: Base Model testing","metadata":{"id":"oOyOKQYQmmlh"}},{"cell_type":"code","source":"%load_ext autotime","metadata":{"id":"hE0siPrJmmlh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the Surprise library, the following algorithms will be used. RMSE is used as the accuracy metric for the predictions:\n\n**NormalPredictor**\n\nAlgorithm predicting a random rating based on the distribution of the training set, which is assumed to be normal.\n\n**BaselineOnly**\n\nAlgorithm predicting the baseline estimate for given user and item.\n\n**KNNBasic**\n\nA basic collaborative filtering algorithm.\n\n**SVD**\n\nThe famous SVD algorithm, as popularized by Simon Funk during the Netflix Prize.\n\n**SVDpp**\n\nThe SVD++ algorithm, an extension of SVD taking into account implicit ratings.\n\n**Nonnegative Matrix Factorization (NMF)**\n\nA collaborative filtering algorithm based on Non-negative Matrix Factorization.\n\n**SlopeOne**\n\nA simple yet accurate collaborative filtering algorithm.\n\n**CoClustering**\n\nA collaborative filtering algorithm based on co-clustering.","metadata":{"id":"Fb988KOsmmli"}},{"cell_type":"markdown","source":"<a id=\"three\"></a>\n## 4.1 Cross-Validation Testing \n<a class=\"anchor\" id=\"1.1\"></a>\n<a href=#cont>Back to Table of Contents</a>","metadata":{"id":"5cj3rhe8mmlj"}},{"cell_type":"markdown","source":"**Cross-Validation Testing**\n\nHere we perform cross-validation testing on five algorithms: SVD, NormalPredictor, BaseLineOnly, SlopeOne and CoClustering.","metadata":{"id":"QBu7sxHNmmlj"}},{"cell_type":"code","source":"# Select sample size of 500 000 to test base models\ndf_model_testing = df_ratings.sample(n=500000)\n\nreader = Reader(rating_scale=(0.5, 5))\ndata = Dataset.load_from_df(df_model_testing[[\"userId\", \"movieId\", \"rating\"]], reader)","metadata":{"id":"HX08v0VGmmlj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We then calculate the RMSEs for the five algorithms and display the scores in a dataframe.","metadata":{"id":"nydBy14Fmmlj"}},{"cell_type":"code","source":"benchmark = []\n\n# Iterate over all algorithms\nfor algorithm in [\n    SVD(),\n    NMF(),\n    NormalPredictor(),\n    BaselineOnly(),\n    SlopeOne(),\n    CoClustering(),\n]:\n\n    # Perform cross validation\n    results = cross_validate(algorithm, data, cv=5)\n\n    # Get results & append algorithm name\n    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n    tmp = tmp.append(\n        pd.Series([str(algorithm).split(\" \")[0].split(\".\")[-1]], index=[\"Algorithm\"])\n    )\n    benchmark.append(tmp)","metadata":{"id":"VPxX1XXImmlk","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show summary dataframe\nsummary_results = (\n    pd.DataFrame(benchmark).set_index(\"Algorithm\").sort_values(\"test_rmse\")\n)\nsummary_results","metadata":{"id":"JUCQUwRWmmlk","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With a dataset of 500 000 rows the BaselineOnly model outperforms the SVD Model. The fit time of the BaselineOnly model is much better than the SVD model. Gridsearch","metadata":{"id":"bui5uVFkmmlk"}},{"cell_type":"markdown","source":"Next we perform split testing.","metadata":{"id":"fi5y3JX-mmlk"}},{"cell_type":"markdown","source":"<a id=\"three\"></a>\n## 4.2 Train-Test-Split\n<a class=\"anchor\" id=\"1.1\"></a>\n<a href=#cont>Back to Table of Contents</a>","metadata":{"id":"hz0SuBY5mmlk"}},{"cell_type":"markdown","source":"**Train-Test-Split Testing of Top 2 Models**\n\nHere we perform a train-test-split test on the BaselineOnly model and SVD model, the top two perfomers.","metadata":{"id":"CZEXVPbHmmlk"}},{"cell_type":"code","source":"# Use all rows in Ratings dataframe\ndata_1 = Dataset.load_from_df(df_ratings[[\"userId\", \"movieId\", \"rating\"]], reader)\n\n# Test set is made of 25% of the ratings.\ntrainset, testset = train_test_split(data_1, test_size=0.25)","metadata":{"id":"QmRn6xOdmmll","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**BaselineOnly Model**\n\nTest the BaseLineOnly model.","metadata":{"id":"5t_VAjcWmmll"}},{"cell_type":"code","source":"BaselineOnly_1 = BaselineOnly()\n\n# Train the algorithm on the train set, and predict ratings for the test set\nBaselineOnly_1.fit(trainset)\npred_Baseline = BaselineOnly_1.test(testset)\n\n# Then compute RMSE\naccuracy.rmse(pred_Baseline)","metadata":{"id":"z8VonK1zmmlm","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**SVD Model**\n\nTest the SVD model.","metadata":{"id":"DVVGhxQTmmlm"}},{"cell_type":"code","source":"SVD_1 = SVD()\n\n# Train the algorithm on the trainset, and predict ratings for the testset\nSVD_1.fit(trainset)\npred_SVD_1 = SVD_1.test(testset)\n\n# Then compute RMSE\naccuracy.rmse(pred_SVD_1)","metadata":{"id":"okz1sVK2mmlm","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"three\"></a>\n## 4.3 GridSearch\n<a class=\"anchor\" id=\"1.1\"></a>\n<a href=#cont>Back to Table of Contents</a>","metadata":{"id":"yDdFLU12mmln"}},{"cell_type":"markdown","source":"**GridSearch**\n\nNext we attempt to improve our model's performance by conducting a grid search on both the SVD and BaselineOnly models.","metadata":{"id":"_G3pkKXUmmln"}},{"cell_type":"markdown","source":"SVD Model","metadata":{"id":"Pp6_i1CWmmln"}},{"cell_type":"code","source":"# # API key to run experiment in Comet\nexperiment = Experiment(\n    api_key=\"RpnzF8DcMSor3mXqAfEQqsXjv\",\n    project_name=\"unsupervised-learning-predict\",\n    workspace=\"teamnm3\",\n)\n\nreader = Reader(rating_scale=(0.5, 5))\ndf_model_testing_3 = df_ratings.sample(n=10000)\ndata_3 = Dataset.load_from_df(\n     df_model_testing_3[[\"userId\", \"movieId\", \"rating\"]], reader\n )\nparam_grid = {\n      \"n_factors\": [10, 20, 50, 100, 150, 200],\n      \"n_epochs\": [15, 20, 25, 50, 75, 100],\n      \"lr_all\": [0.005, 0.008, 0.001],\n      \"reg_all\": [0.1, 0.3, 0.5],\n }\n\ngs = GridSearchCV(SVD, param_grid, measures=[\"rmse\", \"mae\"], cv=3)\ngs.fit(data_3)\nalgo = gs.best_estimator[\"rmse\"]\n\n# # best RMSE score\nprint(gs.best_score[\"rmse\"])\n\n# # combination of parameters that gave the best RMSE score\nprint(gs.best_params[\"rmse\"])\n\nexperiment.log_dataset_hash(data_3)\nexperiment.log_parameters({\"model_type\": \"SVD\", \"param_grid\": param_grid})\nexperiment.log_metrics({\"RMSE\": gs.best_score[\"rmse\"]})\nexperiment.end()","metadata":{"id":"vW1SCUDZmmln","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model Testing with Hyper Parameters Tuned - SVD**\n\nHere we use the set of tuned hyperparameters obtained from experiment to train and test our second SVD model.","metadata":{"id":"GWN54F2zmmlo"}},{"cell_type":"code","source":"# RMSE test whole dataset 1st set of tuned parameters - logged in Comet_ml\nSVD_2 = SVD(n_factors=200, n_epochs=100, lr_all=0.005, reg_all=0.1, random_state=27)\n\n# Train the algorithm on the trainset, and predict ratings for the testset\nSVD_2.fit(trainset)\npred_SVD_2 = SVD_2.test(testset)\n\n# Then compute RMSE\naccuracy.rmse(pred_SVD_2)","metadata":{"id":"2FnCplfcmmlo","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**BaselineOnly Model¶**\n\nWe repeat this process for the BaselineOnly model by running another GridSearchCV.","metadata":{"id":"e22hEUPJmmlo"}},{"cell_type":"code","source":"\n\n# API key to run experiment in Comet\nexperiment = Experiment(\n    api_key=\"RpnzF8DcMSor3mXqAfEQqsXjv\",\n    project_name=\"unsupervised-learning-predict\",\n    workspace=\"teamnm3\",\n)\n\nparam_grid = {\n     \"bsl_options\": {\n         \"method\": [\"sgd\"],\n         \"learning_rate\": [0.004, 0.006, 0.008, 0.010],  # gamma\n         \"reg\": [0.015, 0.020, 0.025],  # lambda 1 and 5\n     }\n }\ngs_baseline = GridSearchCV(\n     BaselineOnly,\n     param_grid,\n     measures=[\"rmse\"],\n     cv=3,\n     return_train_measures=True,\n     n_jobs=1,\n )\ngs_baseline.fit(data_3)\n\nalgo_baseline = gs_baseline.best_estimator[\"rmse\"]\n\n# # best RMSE score\nprint(gs_baseline.best_score[\"rmse\"])\n\n# # combination of parameters that gave the best RMSE score\nprint(gs_baseline.best_params[\"rmse\"])\n\nexperiment.log_dataset_hash(data_3)\nexperiment.log_parameters({\"model_type\": \"BaselineOnly\", \"param_grid\": param_grid})\nexperiment.log_metrics({\"RMSE\": gs_baseline.best_score[\"rmse\"]})\n\nexperiment.end()","metadata":{"id":"U1juHmGhmmlo","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Section 5: Model Building","metadata":{"id":"rFC2AQssmmlq"}},{"cell_type":"markdown","source":"<a id=\"three\"></a>\n## 5.1 Fit model to whole dataset\n<a class=\"anchor\" id=\"1.1\"></a>\n<a href=#cont>Back to Table of Contents</a>","metadata":{"id":"eyb1X8Mymmlq"}},{"cell_type":"markdown","source":"We now switch to training our model on the entire train dataset.","metadata":{"id":"12mqJw8Pmmlr"}},{"cell_type":"code","source":"# Use all rows in Ratings dataframe\ndata = Dataset.load_from_df(df_ratings[[\"userId\", \"movieId\", \"rating\"]], reader)\n\n# Test set is made of 25% of the ratings.\ntrainset, testset = train_test_split(data, test_size=0.25)\n\n# Final Model Building\nSVD_model = SVD(random_state=27)\ntrainset = data.build_full_trainset()\nSVD_model.fit(trainset)","metadata":{"id":"-fpWq685mmlr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"three\"></a>\n## 5.2 Download CSV for Kaggle Competition\n<a class=\"anchor\" id=\"1.1\"></a>\n<a href=#cont>Back to Table of Contents</a>","metadata":{"id":"KEwEER-5mmlr"}},{"cell_type":"code","source":"df_test[\"rating\"] = df_test.apply(\n    lambda x: SVD_model.predict(x[\"userId\"], x[\"movieId\"]).est, axis=1\n)\ndf_test[\"Id\"] = df_test.apply(lambda x: f\"{x['userId']:.0f}_{x['movieId']:.0f}\", axis=1)\nsubmission = df_test[[\"Id\", \"rating\"]]","metadata":{"id":"2oBKx-aPmmlr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"SVD_model.csv\", index=False)","metadata":{"id":"mh7Kew9dmmlr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"three\"></a>\n## 5.3 Pickle model for use in Streamlit\n<a class=\"anchor\" id=\"1.1\"></a>\n<a href=#cont>Back to Table of Contents</a>","metadata":{"id":"Wd_CmGhzmmlr"}},{"cell_type":"code","source":"# Uncomment to pickle and download the final model\n# pickle.dump(SVD_model, open(\"./SVD_model.pkl\",'wb'))","metadata":{"id":"R6lZEK35mmls","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Section 6: Collaborative & Content Based Models","metadata":{"id":"w9iTJk2dmmls"}},{"cell_type":"markdown","source":"**Filtration Strategies for Movie Recommendation Systems**\n\nMovie recommendation systems use a set of different filtration strategies and algorithms to help users find the most relevant films. The most popular categories of the ML algorithms used for movie recommendations include content-based filtering and collaborative filtering systems.\n\n![content-based_vs_collaborative_light.png](attachment:4d15547f-29ec-4040-9af9-5425213778b0.png)","metadata":{"id":"TXsKsafsmmls"}},{"cell_type":"markdown","source":"<a id=\"three\"></a>\n## 6.1 Collaborative Filtering: Approach I\n<a class=\"anchor\" id=\"1.1\"></a>\n<a href=#cont>Back to Table of Contents</a>","metadata":{"id":"SNi5qNZSmmls"}},{"cell_type":"markdown","source":"— **Collaborative Filtering**\n\nAs the name suggests, this filtering strategy is based on the combination of the relevant user’s and other users’ behaviors. The system compares and contrasts these behaviors for the most optimal results. It’s a collaboration of the multiple users’ film preferences and behaviors.\n\nWhat’s the mechanism behind this strategy? The core element in this movie recommendation system and the ML algorithm it’s built on is the history of all users in the database. Basically, collaborative filtering is based on the interaction of all users in the system with the items (movies). Thus, every user impacts the final outcome of this ML-based recommendation system, while content-based filtering depends strictly on the data from one user for its modeling.\n\nCollaborative filtering algorithms are divided into two categories:\n\n- **User-based collaborative filtering**. The idea is to look for similar patterns in movie preferences in the target user and other users in the database.\n- **Item-based collaborative filtering**. The basic concept here is to look for similar items (movies) that target users rate or interact with.\nThe modern approach to the movie recommendation systems implies a mix of both strategies for the most gradual and explicit results.","metadata":{"id":"4fubP1Aymmls"}},{"cell_type":"code","source":"# removing years in title\ndf_movies['title'] = df_movies.title.str.replace('(\\(\\d\\d\\d\\d\\))', '')\ndf_movies['title'] = df_movies['title'].apply(lambda x: x.strip())\ndf_movies.head()","metadata":{"id":"yt5Iv_jsmmls","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We start this process by filtering the user and movie data by users that rated more than 60 movies.","metadata":{"id":"2Xj_9Oi_mmlt"}},{"cell_type":"code","source":"# Convert IDs to int. Required for merging\ndf_ratings['movieId'] = df_ratings['movieId'].astype('int')\ndf_movies['movieId'] = df_movies['movieId'].astype('int')\n\n# Merge df_rating and df_movies into your main dataframe\ntrain_dat = df_ratings.merge(df_movies, on='movieId')","metadata":{"id":"126vbz1Fmmlt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting only the columns will need from merged data\ndf_train = train_dat[['userId','movieId','title','rating']]\ndf_train.head()","metadata":{"id":"WieiaGjRmmlt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert rating into appropriate data types\ndf_train.rating = df_train.rating.astype(str).astype(float)","metadata":{"id":"6kMVL4aPmmlt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show data\ndf_train.head()","metadata":{"id":"e1T5h6demmlt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confirm the number of unique users, unique movies, and total ratings, and we will also calculate the average number of ratings provided by users:\n\nn_users = df_train.userId.unique().shape[0]\nn_movies = df_train.movieId.unique().shape[0]\nn_ratings = len(df_train)\navg_ratings_per_user = n_ratings/n_users\nprint('Number of unique users: ', n_users)\nprint('Number of unique movies: ', n_movies)\nprint('Number of total ratings: ', n_ratings)\nprint('Average number of ratings per user: ', avg_ratings_per_user)","metadata":{"id":"jOUiiSIjmmlt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To reduce the complexity and size of this dataset, we focus on the top one thousand most rated movies.\n\nmovieIndex = df_train.groupby(\"movieId\").count().sort_values(by= \\\n\"rating\",ascending=False)[0:1000].index\nrating2 = df_train[df_train.movieId.isin(movieIndex)]\nrating2.count()","metadata":{"id":"OJm6OeNkmmlt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We will also take a sample of one thousand users at random and filter the dataset for just these users.\n\nuserIndex = rating2.groupby(\"userId\").count().sort_values(by= \\\n\"rating\",ascending=False).sample(n=1000, random_state=2018).index\nrating3 = rating2[rating2.userId.isin(userIndex)]\nrating3.count()","metadata":{"id":"-g9V47rZmmlu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we also reindex movieID and userID to a range of 1 to 1,000 for our reduced dataset\n\nmovies = rating3.movieId.unique()\nmovies_df = pd.DataFrame(data=movies,columns=['originalMovieId'])\nmovies_df['newMovieId'] = movies_df.index+1\nusers = rating3.userId.unique()\nusers_df = pd.DataFrame(data=users,columns=['originalUserId'])\nusers_df['newUserId'] = users_df.index+1\nrating3 = rating3.merge(movies_df,left_on='movieId', \\\nright_on='originalMovieId')\nrating3.drop(labels='originalMovieId', axis=1, inplace=True)\nrating3 = rating3.merge(users_df,left_on='userId', \\\nright_on='originalUserId')\nrating3.drop(labels='originalUserId', axis=1, inplace=True)","metadata":{"id":"3bXO7zOImmlw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let’s calculate the number of unique users, unique movies, total ratings, and average number of ratings per user for our reduced dataset.\n\nn_users = rating3.userId.unique().shape[0]\nn_movies = rating3.movieId.unique().shape[0]\nn_ratings = len(rating3)\navg_ratings_per_user = n_ratings/n_users\nprint('Number of unique users: ', n_users)\nprint('Number of unique movies: ', n_movies)\nprint('Number of total ratings: ', n_ratings)\nprint('Average number of ratings per user: ', avg_ratings_per_user)","metadata":{"id":"7q1TQMIbmmlw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rating3.head()","metadata":{"id":"xtrrUJtAmmlw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we construct our utility matrix easily by using the pivot_table function\n\nutil_matrix = rating3.pivot_table(index=['newUserId'], \n                                       columns=['title'],\n                                       values='rating') \nutil_matrix.shape","metadata":{"id":"ce1iTsmxmmlw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a neat version of the utility matrix to assist with plotting book titles \nrating3['neat_title'] = rating3['title'].apply(lambda x: x[:20])\nutil_matrix_neat = rating3.pivot_table(index=['newUserId'], \n                                            columns=['neat_title'],\n                                            values='rating')\n\nfig, ax = plt.subplots(figsize=(15,5))\n# We select only the first 100 users for ease of computation and visualisation. \n# You can play around with this value to see more of the utility matrix. \n_ = sns.heatmap(util_matrix_neat[:100], annot=False, ax=ax).set_title('Movies Utility Matrix')","metadata":{"id":"Ge0Zj8ECmmlx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize each row (a given user's ratings) of the utility matrix\nutil_matrix_norm = util_matrix.apply(lambda x: (x-np.mean(x))/(np.max(x)-np.min(x)), axis=1)\n# Fill Nan values with 0's, transpose matrix, and drop users with no ratings\nutil_matrix_norm.fillna(0, inplace=True)\nutil_matrix_norm = util_matrix_norm.T\nutil_matrix_norm = util_matrix_norm.loc[:, (util_matrix_norm != 0).any(axis=0)]\n# Save the utility matrix in scipy's sparse matrix format\nutil_matrix_sparse = sp.sparse.csr_matrix(util_matrix_norm.values)","metadata":{"id":"ON9sbf6Wmmlx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute the similarity matrix using the cosine similarity metric\nuser_similarity = cosine_similarity(util_matrix_sparse.T)\n# Save the matrix as a dataframe to allow for easier indexing  \nuser_sim_df = pd.DataFrame(user_similarity, \n                           index = util_matrix_norm.columns, \n                           columns = util_matrix_norm.columns)\n\n# Review a small portion of the constructed similartiy matrix  \nuser_sim_df[:5]","metadata":{"id":"MXfagNKRmmlx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collab_generate_top_N_recommendations(user, N=10, k=20):\n    # Cold-start problem - no ratings given by the reference user. \n    # With no further user data, we solve this by simply recommending\n    # the top-N most popular books in the item catalog. \n    if user not in user_sim_df.columns:\n        return rating3.groupby('title').mean().sort_values(by='rating',\n                                        ascending=False).index[:N].to_list()\n    \n    # Gather the k users which are most similar to the reference user \n    sim_users = user_sim_df.sort_values(by=user, ascending=False).index[1:k+1]\n    favorite_user_items = [] # <-- List of highest rated items gathered from the k users  \n    most_common_favorites = {} # <-- Dictionary of highest rated items in common for the k users\n    \n    for i in sim_users:\n        # Maximum rating given by the current user to an item \n        max_score = util_matrix_norm.loc[:, i].max()\n        # Save the names of items maximally rated by the current user   \n        favorite_user_items.append(util_matrix_norm[util_matrix_norm.loc[:, i]==max_score].index.tolist())\n        \n    # Loop over each user's favorite items and tally which ones are \n    # most popular overall.\n    for item_collection in range(len(favorite_user_items)):\n        for item in favorite_user_items[item_collection]: \n            if item in most_common_favorites:\n                most_common_favorites[item] += 1\n            else:\n                most_common_favorites[item] = 1\n    # Sort the overall most popular items and return the top-N instances\n    sorted_list = sorted(most_common_favorites.items(), key=operator.itemgetter(1), reverse=True)[:N]\n    top_N = [x[0] for x in sorted_list]\n    return top_N","metadata":{"id":"mffIpl-Emmlx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Our recommended list for user 41\ncollab_generate_top_N_recommendations(41)","metadata":{"id":"8KAJYsD_mmly","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# User 41's historical ratings. only 10 shown\nrating3[rating3['newUserId'] == 41][:][['title','rating']].sort_values(by='rating', ascending=False)[:10]","metadata":{"id":"AVWapE7Mmmly","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collab_generate_rating_estimate(movie_title, user, k=20, threshold=0.0):\n    # Gather the k users which are most similar to the reference user \n    sim_users = user_sim_df.sort_values(by=user, ascending=False).index[1:k+1]\n    # Store the corresponding user's similarity values \n    user_values = user_sim_df.sort_values(by=user, ascending=False).loc[:,user].tolist()[1:k+1]\n    rating_list = [] # <-- List of k user's ratings for the reference item\n    weight_list = [] # <-- List of k user's similarities to the reference user\n    \n    # Create a weighted sum for each of the k users who have rated the \n    # reference item (book).\n    for sim_idx, user_id in enumerate(sim_users):\n        # User's rating of the item\n        rating = util_matrix.loc[user_id, movie_title]\n        # User's similarity to the reference user \n        similarity = user_values[sim_idx]\n        # Skip the user if they have not rated the item, or are too dissimilar to \n        # the reference user\n        if (np.isnan(rating)) or (similarity < threshold):\n            continue\n        elif not np.isnan(rating):\n            rating_list.append(rating*similarity)\n            weight_list.append(similarity)\n    try:\n        # Return the weighted sum as the predicted rating for the reference item\n        predicted_rating = sum(rating_list)/sum(weight_list) \n    except ZeroDivisionError:\n        # If no ratings for the reference item can be collected, return the average \n        # rating given by all users for the item.  \n        predicted_rating = np.mean(util_matrix[movie_title])\n    return predicted_rating","metadata":{"id":"_HWexMsTmmly","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Once again we can use our newly formed function to generate rating predictions for user 41\n# pick a movie title 'Heat'\ntitle = \"Heat\"\nactual_rating = rating3[(rating3['newUserId'] == 41) & (rating3['title'] == title)]['rating'].values[0]\npred_rating = collab_generate_rating_estimate(movie_title = title, user = 41)\nprint (f\"Title - {title}\")\nprint (\"---\")\nprint (f\"Actual rating: \\t\\t {actual_rating}\")\nprint (f\"Predicted rating: \\t {pred_rating}\")","metadata":{"id":"cMw61NSUmmly","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we picked a movie title \"Goodfellas\"  and compared ratings\n\ntitle = \"Goodfellas\"\nactual_rating = rating3[(rating3['newUserId'] == 41) & (rating3['title'] == title)]['rating'].values[0]\npred_rating = collab_generate_rating_estimate(movie_title = title, user = 41)\nprint (f\"Title - {title}\")\nprint (\"---\")\nprint (f\"Actual rating: \\t\\t {actual_rating}\")\nprint (f\"Predicted rating: \\t {pred_rating}\")","metadata":{"id":"KqzRaq72mmly","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we picked a movie titled \"Reservoir Dogs\"  and compared ratings\n\ntitle = \"Reservoir Dogs\"\nactual_rating = rating3[(rating3['newUserId'] == 41) & (rating3['title'] == title)]['rating'].values[0]\npred_rating = collab_generate_rating_estimate(movie_title = title, user = 41)\nprint (f\"Title - {title}\")\nprint (\"---\")\nprint (f\"Actual rating: \\t\\t {actual_rating}\")\nprint (f\"Predicted rating: \\t {pred_rating}\")","metadata":{"id":"7YN2mrB_mmly","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"three\"></a>\n## 6.2 Content Based Filtering: Approach II\n<a class=\"anchor\" id=\"1.1\"></a>\n<a href=#cont>Back to Table of Contents</a>","metadata":{"id":"EbHe8qhNmmlz"}},{"cell_type":"markdown","source":"— **Content-Based Filtering**\n\nA filtration strategy for movie recommendation systems, which uses the data provided about the items (movies). This data plays a crucial role here and is extracted from only one user. An ML algorithm used for this strategy recommends motion pictures that are similar to the user’s preferences in the past. Therefore, the similarity in content-based filtering is generated by the data about the past film selections and likes by only one user.\n\nHow does it work? The recommendation system analyzes the past preferences of the user concerned, and then it uses this information to try to find similar movies. This information is available in the database (e.g., lead actors, director, genre, etc.). After that, the system provides movie recommendations for the user. That said, the core element in content-based filtering is only the data of only one user that is used to make predictions.","metadata":{"id":"YD7mHY7mmmlz"}},{"cell_type":"markdown","source":"**Join Data Sets**\n\nHere we join all the datasets together into one combined dataframe","metadata":{"id":"aI9Wgn72mmlz"}},{"cell_type":"code","source":"# Join Ratings and Movies Data Sets\ndf_combined = pd.merge(df_ratings, df_movies, on=\"movieId\", how=\"left\")\n\n# Join IMDB Data Set as well\ndf_combined = pd.merge(df_combined, df_imdb, on=\"movieId\", how=\"left\")\n\n# Display top 5 rows of new combined dataframe\ndf_combined.head(2)","metadata":{"id":"mrsGM1knmmlz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We start this process by filtering the user and movie data by users that rated more than 60 movies.","metadata":{"id":"vRwdIz_9mmlz"}},{"cell_type":"code","source":"# Filter ratings with users that rated more than 60 times to be able to pivot \n# the table\nfiltered_ratings = df_ratings.groupby(\"userId\").filter(lambda x: len(x) >= 60)\n\n# List the movie titles after filtering\nmovie_list_rating = filtered_ratings.movieId.unique().tolist()\n\n# View shape of filtered ratings\nprint(filtered_ratings.shape)","metadata":{"id":"uCqeKC0zmmlz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we examine the percentage prevelance of unique movies in the filtered_ratings dataframe that was created above:","metadata":{"id":"49SqYQCdmml0"}},{"cell_type":"code","source":"# Calculate percentage of movies and users in filtered dataframe\nunique_movies_f = (\n    len(filtered_ratings.movieId.unique()) / len(df_ratings.movieId.unique()) * 100\n)\nunique_users_f = (\n    len(filtered_ratings.userId.unique()) / len(df_ratings.userId.unique()) * 100\n)\n\nprint(\n    round(unique_movies_f, 2), \"% of original movie titles in the filtered dataframe.\"\n)\nprint(round(unique_users_f, 2), \"% of original users in the filtered dataframe.\")","metadata":{"id":"xSiMn_OGmml0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we understand the content of this dataframe, we proceed to filter the movies dataframe to exlude items not in the movie_list_rating and process the genres column:","metadata":{"id":"0nipee1emml0"}},{"cell_type":"code","source":"# Filter the movies dataframe with the movie titles from the filtered list\ndf_movies = df_movies[df_movies.movieId.isin(movie_list_rating)]","metadata":{"id":"ZZeCdpbDmml0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We then process the genres column to remove the pipe symbol:","metadata":{"id":"Zka9aTynmml1"}},{"cell_type":"code","source":"# Replace | in genres with a space and make lowercase to use as metadata later\ndf_movies[\"genres\"] = [item.replace(\"|\", \" \").lower() for item in df_movies[\"genres\"]]\ndf_movies.head()","metadata":{"id":"J_thHTf9mml1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We continue to process this dataframe by dropping the timestamp column and mappign the movie title to its ID.","metadata":{"id":"1UZ4_j9Pmml2"}},{"cell_type":"code","source":"# Create movie dictionary to map title to id\nmovie_dict = dict(zip(df_movies.title.tolist(), df_movies.movieId.tolist()))\n\n# Drop timestamp column from filtered dataframe\nfiltered_ratings.drop([\"timestamp\"], axis=1, inplace=True)\nfiltered_ratings.head()","metadata":{"id":"iN-_EqlYmml3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we add tags with a high relevance score (above 0.7)","metadata":{"id":"hhSQPs17mml3"}},{"cell_type":"code","source":"# Create combined dataframe with genres, titles and tags with relevance above 0.7\ncombined = pd.merge(df_movies, df_genome_scores, on=\"movieId\", how=\"left\").merge(\n    df_genome_tags, on=\"tagId\", how=\"left\"\n)\n\nfilter_combined = combined[combined[\"relevance\"] > 0.9]\nfilter_combined.drop([\"tagId\", \"relevance\"], axis=1, inplace=True)\n\n# Replace NaN with empty string\nfilter_combined.fillna(\"\", inplace=True)\nfilter_combined.head()","metadata":{"id":"6Gimj9MLmml4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we're able to create a new metadata column in the dataframe by concatenating the tag with the genres.","metadata":{"id":"9Hg_Ppwtmml4"}},{"cell_type":"code","source":"# Create metadata column from movie tags and genres\nfilter_combined = pd.DataFrame(\n    filter_combined.groupby(\"movieId\")[\"tag\"].apply(lambda x: \" \".join(x))\n)\n\nmovies_meta = pd.merge(df_movies, filter_combined, on=\"movieId\", how=\"left\").fillna(\"\")\nmovies_meta[\"metadata\"] = movies_meta[\"tag\"] + \" \" + movies_meta[\"genres\"]\n\nmovies_meta[[\"movieId\", \"title\", \"metadata\"]].head()","metadata":{"id":"hEnPEuM7mml4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convienient indexes to map between book titles and indexes of \n# the books dataframe\ntitles = movies_meta['title']\nindices = pd.Series(movies_meta.index, index=movies_meta['title'])","metadata":{"id":"siT-IYslmml4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show indices\n\nindices.head()","metadata":{"id":"YLihwxUXmml4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now convert these textual features into a format which enables us to compute their relative similarities to one another.","metadata":{"id":"Oet19Jk8mml4"}},{"cell_type":"code","source":"tf = TfidfVectorizer(analyzer='word', ngram_range=(1,2),\n                     min_df=0, stop_words='english')\n\n# Produce a feature matrix, where each row corresponds to a book,\n# with TF-IDF features as columns \ntf_authTags_matrix = tf.fit_transform(movies_meta['metadata'])\n\ndf_tfidf = pd.DataFrame(tf_authTags_matrix.toarray(), index=movies_meta.index.tolist())\n\nprint(df_tfidf.shape)","metadata":{"id":"uJOekVwKmml5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select top features with Truncated SVD\nsvd = TruncatedSVD(n_components=200)\nlatent_matrix = svd.fit_transform(df_tfidf)\n\n# Plot variance expalained to see what dimensions to use\nexplained = svd.explained_variance_ratio_.cumsum()\nplt.plot(explained, \".-\", ms=10, color=\"blue\")\nplt.xlabel(\"Singular value components\", fontsize=12)\nplt.ylabel(\"Cumulative percent of variance\", fontsize=12)\nplt.show()\n\n# Print percentage of variance explained\nprint(\n    \"200 components explains\",\n    round(svd.explained_variance_ratio_.sum(), 2) * 100,\n    \"% of the variance\",\n)","metadata":{"id":"fb6CLT-Fmml5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of dimensions to keep\nn = 200\ndf_latent_matrix_1 = pd.DataFrame(\n    latent_matrix[:, 0:n], index=movies_meta.title.tolist()\n)\n\n# Content latent matrix shape\nlatent_matrix.shape","metadata":{"id":"4p_U7Vu-mml5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now can compute the similarity between each vector within our matrix. This is done by making use of the cosine_similarity function provided to us by sklearn.","metadata":{"id":"o-yBPp1wmml5"}},{"cell_type":"code","source":"cosine_sim_authTags = cosine_similarity(latent_matrix, \n                                        latent_matrix)\nprint (cosine_sim_authTags.shape)","metadata":{"id":"GrAYG1pvmml5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cosine_sim_authTags[:5]","metadata":{"id":"wFjC6vJlmml5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Infinity One\nBuilding a model with more Attributes for Streamlit (Infinity One)","metadata":{"id":"8YIZzvK1pcBg"}},{"cell_type":"code","source":"# Convert IDs to int. Required for merging\n# Convert 'movieIds' to int. This is necessary for merging datasets\ndf_movies['movieId'] = df_movies['movieId'].astype('int')\ndf_imdb['movieId'] = df_imdb['movieId'].astype('int')\n\n# Merge df_imdb and df_mov into the main items_dataset\nitems_dataset = df_imdb.merge(df_movies, on='movieId')\n#items_dataset.head(5)","metadata":{"id":"t568hJU0pbEF","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract necessary elements from the columns within the items_dataset\n# view - title_cast, director, budget, plot_keywords, title, genres\n# and strip spaces in names to avoid similar names counted equal\n        \nelements = ['title_cast', 'plot_keywords', 'genres'] \nfor item in elements:\n    items_dataset[item] = items_dataset[item].fillna('')\n    items_dataset[item] = items_dataset[item].str.replace(' ', '')\n    items_dataset[item] = items_dataset[item].str.replace('|', ' ')\n    \n# create a year column from the title column of the dataset\nitems_dataset['year'] = items_dataset.title.str.extract('(\\d+)')\nitems_dataset['year'] = items_dataset['year'].fillna(np.nan)\nitems_dataset['runtime'] = items_dataset['runtime'].fillna(np.nan)\n\n# then remove year from title column\nitems_dataset['title'] = items_dataset.title.str.replace('(\\(\\d\\d\\d\\d\\))', '')\nitems_dataset['title'] = items_dataset.title.str.strip()","metadata":{"id":"jf87KPd5rsdj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop unwanted columns\nitems_dataset = items_dataset.drop(['budget','movieId'], axis=1)","metadata":{"id":"TPGyr4Yxq-EO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clean up the dataset by converting all to lower case \n# prepared for vectorizer\n\ndef clean_items(x):\n    if isinstance(x, str):\n        res = str.lower(x)\n        return res \n    else:\n        return ''","metadata":{"id":"nIhFxdWbsDQx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply function to items_dataset columns\n\nelements2 = ['title_cast', 'director', 'plot_keywords', 'genres'] \nfor item in elements2:\n    items_dataset[item] = items_dataset[item].apply(clean_items)","metadata":{"id":"5gHd5yiHsuNT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make columns strings to add to vectorizer output\nitems_dataset['runtime'] = items_dataset['runtime'].apply(lambda x: str(x))\nitems_dataset['year'] = items_dataset['year'].apply(lambda x: str(x))","metadata":{"id":"_ctMhN1Dswrd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# elements = ['title_cast', 'director', 'plot_keywords', 'genres'] \n# from elements, create final preprocessing of items_pool, \n# get ready for vectorizer\ndef items_pool(x):\n    return x['plot_keywords']           \\\n                + ' ' + x['title_cast'] \\\n                + ' ' + x['director']   \\\n                + ' ' + x['genres']     \\\n                + ' ' + x['runtime']    \\\n                + ' ' + x['year']\n","metadata":{"id":"XaOWYcbAtFY1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a new column into our items_dataset for the items_pool\nitems_dataset['pool'] = items_dataset.apply(items_pool, axis=1)","metadata":{"id":"zajvYNpAtJjn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### RECOMMENDER STEPS\n1. Using the index mapping, obtain the index of the movie given its title.\n\n2. Create a list of tuples containing the index number of the movies and the cosine similarity scores for that particular movie compared with all movies. \n\n3. Sort the list of tuples based on the similarity scores; that is, the second element of the tuple.\n\n4. Get the top N elements of this list. Ignore the first element as it refers to self (the movie most similar to a particular movie is the movie itself).\n\n5. Return the titles corresponding to the indices of the top N elements.","metadata":{"id":"a2O_WzDdudeV"}},{"cell_type":"code","source":"# make CountVectorizer object and create the count matrix\n# using countvectorizer to preserve the weights of important elements\ncount_vect = CountVectorizer(stop_words='english')\ncount_matrix = count_vect.fit_transform(items_dataset['pool'])","metadata":{"id":"R4H76nfRtRgh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Dimensionality Reduction to get the most important feature sets","metadata":{"id":"rMwMTYrhxBso"}},{"cell_type":"code","source":"# defining global scaler objects\nss = StandardScaler()\n\n# determine features and drop some columns\n#labels = []\n# scale the dataframe\ncount_matrix_scaled = ss.fit_transform(count_matrix)","metadata":{"id":"tSEYs4qmv59F","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# carrying out dimensionality reduction\n# apply PCA to get the 20 most important feature sets\nprint(\"Computing PCA projection\")\nt0 = time()\ncount_matrix_pca = decomposition.PCA(n_components=20).fit_transform(count_matrix_scaled)\nt1 = time()\nprint(\"Finished PCA projection in \" + str(t1-t0) + \"s.\")","metadata":{"id":"a2Z4fNn4v6Ml","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute the Cosine Similarity matrix based on the count_matrix\ncosine_sim = cosine_similarity(count_matrix_pca, count_matrix_pca)","metadata":{"id":"i37MmHaityKF","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set index of the main DataFrame and \n# construct a reverse mapping of the index to titles\nindices = pd.Series(\n    items_dataset.index, index=items_dataset['title'])","metadata":{"id":"qysK3guvt9Id","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function that takes in movie title as input and outputs most similar movies\ndef get_movie_recs(N, sample, cosine_sim=cosine_sim):\n    '''\n    title: str\n    N: int\n    cosine_sim: float\n    '''\n    # Get the index of the movie that matches the title\n    idx = indices[sample]\n\n    # Get the pairwsie similarity scores of all movies with that movie\n    # in a list of tuples, get the similarity scores next to the indices\n    sim_scores = list(enumerate(cosine_sim[idx]))\n\n    # Sort the movies based on the similarity scores\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n\n    # Get the scores of the N most similar movies\n    sim_scores = sim_scores[1:N+1]\n\n    # Get the movie indices\n    movie_indices = [i[0] for i in sim_scores]\n\n    # Return the top N most similar movies\n    res = items_dataset['title'].iloc[movie_indices]\n    return res","metadata":{"id":"-SPU9jSzukCR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get a sample of the movie recommender capabilities of Infinity One\nUser1_likes_Toy_Story = get_movie_recs(10, 'Toy Story', cosine_sim)","metadata":{"id":"mXFjmba1uuRZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Section 7: Conclusion","metadata":{"id":"CfRa_OfOmml5"}},{"cell_type":"markdown","source":"The bigger the choice, the harder it is to make the final decision. This is especially true for modern movie fans, who have thousands of movies to pick from. But thanks to machine learning, we now have recommendation systems based on its complex algorithms and techniques.\n\nToday, movie recommendation systems are widely used by the most popular streaming services, enabling a more personalized experience and increased user satisfaction across the platforms. Why do we need them? It’s estimated that the world cinema has released more than 500,000 movies — a number beyond one person’s control. With such an enormous number of motion pictures to choose from, developing and improving recommendation systems with ML was a crucial step to make this process easier and feasible.\n\nOnce again, ML proves to be a vital technological solution that makes our lives easier. And the more these systems evolve, the more advanced ML techniques we have at our disposal that generate the most accurate content for users and give them what they are looking for.\n\nOn this notebook we constructed such system.We constructed a recommendation algorithm based on content and collaborative filtering, capable of accurately predicting how a user will rate a movie they have not watched yet based on their historical preference.","metadata":{"id":"hZfJgxvSmml6"}}]}